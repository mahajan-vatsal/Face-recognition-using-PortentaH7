\doxysection{Face\+\_\+\+Access\+\_\+inferencing/src/edge-\/impulse-\/sdk/classifier/inferencing\+\_\+engines/onnx\+\_\+tidl.h File Reference}
\hypertarget{onnx__tidl_8h}{}\label{onnx__tidl_8h}\index{Face\_Access\_inferencing/src/edge-\/impulse-\/sdk/classifier/inferencing\_engines/onnx\_tidl.h@{Face\_Access\_inferencing/src/edge-\/impulse-\/sdk/classifier/inferencing\_engines/onnx\_tidl.h}}
{\ttfamily \#include "{}model-\/parameters/model\+\_\+metadata.\+h"{}}\newline
{\ttfamily \#include $<$getopt.\+h$>$}\newline
{\ttfamily \#include $<$iostream$>$}\newline
{\ttfamily \#include $<$cstdarg$>$}\newline
{\ttfamily \#include $<$cstdio$>$}\newline
{\ttfamily \#include $<$fstream$>$}\newline
{\ttfamily \#include $<$string$>$}\newline
{\ttfamily \#include $<$sys/time.\+h$>$}\newline
{\ttfamily \#include $<$list$>$}\newline
{\ttfamily \#include $<$sys/stat.\+h$>$}\newline
{\ttfamily \#include $<$vector$>$}\newline
{\ttfamily \#include "{}itidl\+\_\+rt.\+h"{}}\newline
{\ttfamily \#include $<$onnxruntime/core/session/onnxruntime\+\_\+cxx\+\_\+api.\+h$>$}\newline
{\ttfamily \#include $<$onnxruntime/core/providers/tidl/tidl\+\_\+provider\+\_\+factory.\+h$>$}\newline
{\ttfamily \#include $<$onnxruntime/core/providers/cpu/cpu\+\_\+provider\+\_\+factory.\+h$>$}\newline
{\ttfamily \#include $<$cmath$>$}\newline
{\ttfamily \#include "{}edge-\/impulse-\/sdk/classifier/ei\+\_\+aligned\+\_\+malloc.\+h"{}}\newline
{\ttfamily \#include "{}edge-\/impulse-\/sdk/classifier/ei\+\_\+fill\+\_\+result\+\_\+struct.\+h"{}}\newline
{\ttfamily \#include "{}edge-\/impulse-\/sdk/classifier/ei\+\_\+model\+\_\+types.\+h"{}}\newline
{\ttfamily \#include "{}onnx-\/model/tidl-\/model.\+h"{}}\newline
{\ttfamily \#include "{}utils/model\+\_\+header\+\_\+utils.\+h"{}}\newline
Include dependency graph for onnx\+\_\+tidl.\+h\+:
% FIG 0
\doxysubsubsection*{Macros}
\begin{DoxyCompactItemize}
\item 
\#define \mbox{\hyperlink{onnx__tidl_8h_aec6ca27916f1b9919f2d3517cdca1e16}{TI\+\_\+\+PREPROC\+\_\+\+DEFAULT\+\_\+\+WIDTH}}~320
\item 
\#define \mbox{\hyperlink{onnx__tidl_8h_abb02f72d942578f258e40526d3644471}{TI\+\_\+\+PREPROC\+\_\+\+DEFAULT\+\_\+\+HEIGHT}}~240
\end{DoxyCompactItemize}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
double \mbox{\hyperlink{onnx__tidl_8h_a6bceed6d0d125a16328def7d40c9a8f2}{get\+Us}} (struct timeval t)
\begin{DoxyCompactList}\small\item\em returns time in micro sec \end{DoxyCompactList}\item 
int \mbox{\hyperlink{onnx__tidl_8h_a763b74426e540261d9ac52dd861a77bf}{print\+Tensor\+Info}} (Ort\+::\+Session \texorpdfstring{$\ast$}{*}session, std\+::vector$<$ const char \texorpdfstring{$\ast$}{*} $>$ \texorpdfstring{$\ast$}{*}input\+\_\+node\+\_\+names, std\+::vector$<$ const char \texorpdfstring{$\ast$}{*} $>$ \texorpdfstring{$\ast$}{*}output\+\_\+node\+\_\+names)
\begin{DoxyCompactList}\small\item\em print tensor info \end{DoxyCompactList}\item 
void \texorpdfstring{$\ast$}{*} \mbox{\hyperlink{onnx__tidl_8h_a47ef4bb644fdd5a4924ef6ed3ca8cc09}{alloc\+Tensor\+Mem}} (int \mbox{\hyperlink{hello__world__model_8cc_a439227feff9d7f55384e8780cfc2eb82}{size}}, int \mbox{\hyperlink{nicla__sense__fusion_8ino_ab9319fdba407b0f6ef6285faef51f290}{accel}})
\item 
void \mbox{\hyperlink{onnx__tidl_8h_aad1acb9d97076199c3a4476df5b79119}{free\+Tensor\+Mem}} (void \texorpdfstring{$\ast$}{*}\mbox{\hyperlink{ei__aligned__malloc_8h_a0619a76ea2d4e6cce18b93ddf5082a36}{ptr}}, int \mbox{\hyperlink{nicla__sense__fusion_8ino_ab9319fdba407b0f6ef6285faef51f290}{accel}})
\item 
static \mbox{\hyperlink{group__ei__returntypes_gad9580b47a6cd5e74cfc31a03bdfecebe}{EI\+\_\+\+IMPULSE\+\_\+\+ERROR}} \mbox{\hyperlink{onnx__tidl_8h_a1f36bc1c96f3a91d32ede90f7afff0ba}{inference\+\_\+onnx\+\_\+setup}} (const \mbox{\hyperlink{ei__model__types_8h_a936cf405a057a578f2bc7b540ccafe57}{ei\+\_\+impulse\+\_\+t}} \texorpdfstring{$\ast$}{*}impulse, uint64\+\_\+t \texorpdfstring{$\ast$}{*}ctx\+\_\+start\+\_\+us, std\+::vector$<$ Ort\+::\+Value $>$ \texorpdfstring{$\ast$}{*}input\+\_\+tensors, std\+::vector$<$ Ort\+::\+Value $>$ \texorpdfstring{$\ast$}{*}output\+\_\+tensors, Ort\+::\+Session \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}session\+\_\+ptr, Ort\+::\+Run\+Options \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}run\+\_\+options\+\_\+ptr, Ort\+::\+Io\+Binding \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}binding\+\_\+ptr)
\item 
static \mbox{\hyperlink{group__ei__returntypes_gad9580b47a6cd5e74cfc31a03bdfecebe}{EI\+\_\+\+IMPULSE\+\_\+\+ERROR}} \mbox{\hyperlink{onnx__tidl_8h_a01c96c6da283198d0dc9ee34886e233a}{inference\+\_\+onnx\+\_\+run}} (const \mbox{\hyperlink{ei__model__types_8h_a936cf405a057a578f2bc7b540ccafe57}{ei\+\_\+impulse\+\_\+t}} \texorpdfstring{$\ast$}{*}impulse, void \texorpdfstring{$\ast$}{*}\mbox{\hyperlink{ei__run__dsp_8h_a0819d397f9fba7b345b75fef1c1f782a}{config\+\_\+ptr}}, uint64\+\_\+t ctx\+\_\+start\+\_\+us, std\+::vector$<$ Ort\+::\+Value $>$ \texorpdfstring{$\ast$}{*}input\+\_\+tensors, std\+::vector$<$ Ort\+::\+Value $>$ \texorpdfstring{$\ast$}{*}output\+\_\+tensors, Ort\+::\+Session \texorpdfstring{$\ast$}{*}session, Ort\+::\+Run\+Options \texorpdfstring{$\ast$}{*}run\+\_\+options, Ort\+::\+Io\+Binding \texorpdfstring{$\ast$}{*}binding, \mbox{\hyperlink{structei__impulse__result__t}{ei\+\_\+impulse\+\_\+result\+\_\+t}} \texorpdfstring{$\ast$}{*}\mbox{\hyperlink{group__ei__functions_gaf4ad914acba713176b1f00a800e781ba}{result}}, bool \mbox{\hyperlink{ei__fill__result__struct_8h_a881f62341b5a41a3c5707268bd537be1}{debug}})
\item 
\mbox{\hyperlink{group__ei__returntypes_gad9580b47a6cd5e74cfc31a03bdfecebe}{EI\+\_\+\+IMPULSE\+\_\+\+ERROR}} \mbox{\hyperlink{onnx__tidl_8h_ab7a2955e2cf72f882991f1c82cac9c74}{run\+\_\+nn\+\_\+inference}} (const \mbox{\hyperlink{ei__model__types_8h_a936cf405a057a578f2bc7b540ccafe57}{ei\+\_\+impulse\+\_\+t}} \texorpdfstring{$\ast$}{*}impulse, \mbox{\hyperlink{structei__feature__t}{ei\+\_\+feature\+\_\+t}} \texorpdfstring{$\ast$}{*}afmatrix, uint32\+\_\+t learn\+\_\+block\+\_\+index, uint32\+\_\+t \texorpdfstring{$\ast$}{*}input\+\_\+block\+\_\+ids, uint32\+\_\+t input\+\_\+block\+\_\+ids\+\_\+size, \mbox{\hyperlink{structei__impulse__result__t}{ei\+\_\+impulse\+\_\+result\+\_\+t}} \texorpdfstring{$\ast$}{*}\mbox{\hyperlink{group__ei__functions_gaf4ad914acba713176b1f00a800e781ba}{result}}, void \texorpdfstring{$\ast$}{*}\mbox{\hyperlink{ei__run__dsp_8h_a0819d397f9fba7b345b75fef1c1f782a}{config\+\_\+ptr}}, bool \mbox{\hyperlink{ei__fill__result__struct_8h_a881f62341b5a41a3c5707268bd537be1}{debug}}=false)
\begin{DoxyCompactList}\small\item\em Do neural network inferencing over the processed feature matrix. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Macro Definition Documentation}
\Hypertarget{onnx__tidl_8h_abb02f72d942578f258e40526d3644471}\index{onnx\_tidl.h@{onnx\_tidl.h}!TI\_PREPROC\_DEFAULT\_HEIGHT@{TI\_PREPROC\_DEFAULT\_HEIGHT}}
\index{TI\_PREPROC\_DEFAULT\_HEIGHT@{TI\_PREPROC\_DEFAULT\_HEIGHT}!onnx\_tidl.h@{onnx\_tidl.h}}
\doxysubsubsection{\texorpdfstring{TI\_PREPROC\_DEFAULT\_HEIGHT}{TI\_PREPROC\_DEFAULT\_HEIGHT}}
{\footnotesize\ttfamily \label{onnx__tidl_8h_abb02f72d942578f258e40526d3644471} 
\#define TI\+\_\+\+PREPROC\+\_\+\+DEFAULT\+\_\+\+HEIGHT~240}

\Hypertarget{onnx__tidl_8h_aec6ca27916f1b9919f2d3517cdca1e16}\index{onnx\_tidl.h@{onnx\_tidl.h}!TI\_PREPROC\_DEFAULT\_WIDTH@{TI\_PREPROC\_DEFAULT\_WIDTH}}
\index{TI\_PREPROC\_DEFAULT\_WIDTH@{TI\_PREPROC\_DEFAULT\_WIDTH}!onnx\_tidl.h@{onnx\_tidl.h}}
\doxysubsubsection{\texorpdfstring{TI\_PREPROC\_DEFAULT\_WIDTH}{TI\_PREPROC\_DEFAULT\_WIDTH}}
{\footnotesize\ttfamily \label{onnx__tidl_8h_aec6ca27916f1b9919f2d3517cdca1e16} 
\#define TI\+\_\+\+PREPROC\+\_\+\+DEFAULT\+\_\+\+WIDTH~320}



\doxysubsection{Function Documentation}
\Hypertarget{onnx__tidl_8h_a47ef4bb644fdd5a4924ef6ed3ca8cc09}\index{onnx\_tidl.h@{onnx\_tidl.h}!allocTensorMem@{allocTensorMem}}
\index{allocTensorMem@{allocTensorMem}!onnx\_tidl.h@{onnx\_tidl.h}}
\doxysubsubsection{\texorpdfstring{allocTensorMem()}{allocTensorMem()}}
{\footnotesize\ttfamily \label{onnx__tidl_8h_a47ef4bb644fdd5a4924ef6ed3ca8cc09} 
void \texorpdfstring{$\ast$}{*} alloc\+Tensor\+Mem (\begin{DoxyParamCaption}\item[{int}]{size}{, }\item[{int}]{accel}{}\end{DoxyParamCaption})}

\Hypertarget{onnx__tidl_8h_aad1acb9d97076199c3a4476df5b79119}\index{onnx\_tidl.h@{onnx\_tidl.h}!freeTensorMem@{freeTensorMem}}
\index{freeTensorMem@{freeTensorMem}!onnx\_tidl.h@{onnx\_tidl.h}}
\doxysubsubsection{\texorpdfstring{freeTensorMem()}{freeTensorMem()}}
{\footnotesize\ttfamily \label{onnx__tidl_8h_aad1acb9d97076199c3a4476df5b79119} 
void free\+Tensor\+Mem (\begin{DoxyParamCaption}\item[{void \texorpdfstring{$\ast$}{*}}]{ptr}{, }\item[{int}]{accel}{}\end{DoxyParamCaption})}

\Hypertarget{onnx__tidl_8h_a6bceed6d0d125a16328def7d40c9a8f2}\index{onnx\_tidl.h@{onnx\_tidl.h}!getUs@{getUs}}
\index{getUs@{getUs}!onnx\_tidl.h@{onnx\_tidl.h}}
\doxysubsubsection{\texorpdfstring{getUs()}{getUs()}}
{\footnotesize\ttfamily \label{onnx__tidl_8h_a6bceed6d0d125a16328def7d40c9a8f2} 
double get\+Us (\begin{DoxyParamCaption}\item[{struct timeval}]{t}{}\end{DoxyParamCaption})}



returns time in micro sec 

\begin{DoxyReturn}{Returns}
void 
\end{DoxyReturn}
\Hypertarget{onnx__tidl_8h_a01c96c6da283198d0dc9ee34886e233a}\index{onnx\_tidl.h@{onnx\_tidl.h}!inference\_onnx\_run@{inference\_onnx\_run}}
\index{inference\_onnx\_run@{inference\_onnx\_run}!onnx\_tidl.h@{onnx\_tidl.h}}
\doxysubsubsection{\texorpdfstring{inference\_onnx\_run()}{inference\_onnx\_run()}}
{\footnotesize\ttfamily \label{onnx__tidl_8h_a01c96c6da283198d0dc9ee34886e233a} 
static \mbox{\hyperlink{group__ei__returntypes_gad9580b47a6cd5e74cfc31a03bdfecebe}{EI\+\_\+\+IMPULSE\+\_\+\+ERROR}} inference\+\_\+onnx\+\_\+run (\begin{DoxyParamCaption}\item[{const \mbox{\hyperlink{ei__model__types_8h_a936cf405a057a578f2bc7b540ccafe57}{ei\+\_\+impulse\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{impulse}{, }\item[{void \texorpdfstring{$\ast$}{*}}]{config\+\_\+ptr}{, }\item[{uint64\+\_\+t}]{ctx\+\_\+start\+\_\+us}{, }\item[{std\+::vector$<$ Ort\+::\+Value $>$ \texorpdfstring{$\ast$}{*}}]{input\+\_\+tensors}{, }\item[{std\+::vector$<$ Ort\+::\+Value $>$ \texorpdfstring{$\ast$}{*}}]{output\+\_\+tensors}{, }\item[{Ort\+::\+Session \texorpdfstring{$\ast$}{*}}]{session}{, }\item[{Ort\+::\+Run\+Options \texorpdfstring{$\ast$}{*}}]{run\+\_\+options}{, }\item[{Ort\+::\+Io\+Binding \texorpdfstring{$\ast$}{*}}]{binding}{, }\item[{\mbox{\hyperlink{structei__impulse__result__t}{ei\+\_\+impulse\+\_\+result\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{result}{, }\item[{bool}]{debug}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [static]}}

Run ONNX model


\begin{DoxyParams}{Parameters}
{\em ctx\+\_\+start\+\_\+us} & Start time of the setup function (see above) \\
\hline
{\em output\+\_\+tensors} & Output tensors \\
\hline
{\em session} & ONNX session \\
\hline
{\em run\+\_\+options} & ONNX run options \\
\hline
{\em binding} & IO bindings \\
\hline
{\em debug} & Whether to print debug info\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
EI\+\_\+\+IMPULSE\+\_\+\+OK if successful 
\end{DoxyReturn}

\begin{DoxyItemize}
\item freeing shared mem\texorpdfstring{$\ast$}{*}/
\end{DoxyItemize}\Hypertarget{onnx__tidl_8h_a1f36bc1c96f3a91d32ede90f7afff0ba}\index{onnx\_tidl.h@{onnx\_tidl.h}!inference\_onnx\_setup@{inference\_onnx\_setup}}
\index{inference\_onnx\_setup@{inference\_onnx\_setup}!onnx\_tidl.h@{onnx\_tidl.h}}
\doxysubsubsection{\texorpdfstring{inference\_onnx\_setup()}{inference\_onnx\_setup()}}
{\footnotesize\ttfamily \label{onnx__tidl_8h_a1f36bc1c96f3a91d32ede90f7afff0ba} 
static \mbox{\hyperlink{group__ei__returntypes_gad9580b47a6cd5e74cfc31a03bdfecebe}{EI\+\_\+\+IMPULSE\+\_\+\+ERROR}} inference\+\_\+onnx\+\_\+setup (\begin{DoxyParamCaption}\item[{const \mbox{\hyperlink{ei__model__types_8h_a936cf405a057a578f2bc7b540ccafe57}{ei\+\_\+impulse\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{impulse}{, }\item[{uint64\+\_\+t \texorpdfstring{$\ast$}{*}}]{ctx\+\_\+start\+\_\+us}{, }\item[{std\+::vector$<$ Ort\+::\+Value $>$ \texorpdfstring{$\ast$}{*}}]{input\+\_\+tensors}{, }\item[{std\+::vector$<$ Ort\+::\+Value $>$ \texorpdfstring{$\ast$}{*}}]{output\+\_\+tensors}{, }\item[{Ort\+::\+Session \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}}]{session\+\_\+ptr}{, }\item[{Ort\+::\+Run\+Options \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}}]{run\+\_\+options\+\_\+ptr}{, }\item[{Ort\+::\+Io\+Binding \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}}]{binding\+\_\+ptr}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [static]}}

Setup the ONNX runtime


\begin{DoxyParams}{Parameters}
{\em ctx\+\_\+start\+\_\+us} & Pointer to the start time \\
\hline
{\em input} & Pointer to input tensor \\
\hline
{\em output} & Pointer to output tensor \\
\hline
{\em micro\+\_\+interpreter} & Pointer to interpreter (for non-\/compiled models) \\
\hline
{\em micro\+\_\+tensor\+\_\+arena} & Pointer to the arena that will be allocated\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
EI\+\_\+\+IMPULSE\+\_\+\+OK if successful 
\end{DoxyReturn}
\Hypertarget{onnx__tidl_8h_a763b74426e540261d9ac52dd861a77bf}\index{onnx\_tidl.h@{onnx\_tidl.h}!printTensorInfo@{printTensorInfo}}
\index{printTensorInfo@{printTensorInfo}!onnx\_tidl.h@{onnx\_tidl.h}}
\doxysubsubsection{\texorpdfstring{printTensorInfo()}{printTensorInfo()}}
{\footnotesize\ttfamily \label{onnx__tidl_8h_a763b74426e540261d9ac52dd861a77bf} 
int print\+Tensor\+Info (\begin{DoxyParamCaption}\item[{Ort\+::\+Session \texorpdfstring{$\ast$}{*}}]{session}{, }\item[{std\+::vector$<$ const char \texorpdfstring{$\ast$}{*} $>$ \texorpdfstring{$\ast$}{*}}]{input\+\_\+node\+\_\+names}{, }\item[{std\+::vector$<$ const char \texorpdfstring{$\ast$}{*} $>$ \texorpdfstring{$\ast$}{*}}]{output\+\_\+node\+\_\+names}{}\end{DoxyParamCaption})}



print tensor info 


\begin{DoxyParams}{Parameters}
{\em session} & onnx session \\
\hline
{\em input\+\_\+node\+\_\+names} & input array node names \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
int status 
\end{DoxyReturn}
\Hypertarget{onnx__tidl_8h_ab7a2955e2cf72f882991f1c82cac9c74}\index{onnx\_tidl.h@{onnx\_tidl.h}!run\_nn\_inference@{run\_nn\_inference}}
\index{run\_nn\_inference@{run\_nn\_inference}!onnx\_tidl.h@{onnx\_tidl.h}}
\doxysubsubsection{\texorpdfstring{run\_nn\_inference()}{run\_nn\_inference()}}
{\footnotesize\ttfamily \label{onnx__tidl_8h_ab7a2955e2cf72f882991f1c82cac9c74} 
\mbox{\hyperlink{group__ei__returntypes_gad9580b47a6cd5e74cfc31a03bdfecebe}{EI\+\_\+\+IMPULSE\+\_\+\+ERROR}} run\+\_\+nn\+\_\+inference (\begin{DoxyParamCaption}\item[{const \mbox{\hyperlink{ei__model__types_8h_a936cf405a057a578f2bc7b540ccafe57}{ei\+\_\+impulse\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{impulse}{, }\item[{\mbox{\hyperlink{structei__feature__t}{ei\+\_\+feature\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{afmatrix}{, }\item[{uint32\+\_\+t}]{learn\+\_\+block\+\_\+index}{, }\item[{uint32\+\_\+t \texorpdfstring{$\ast$}{*}}]{input\+\_\+block\+\_\+ids}{, }\item[{uint32\+\_\+t}]{input\+\_\+block\+\_\+ids\+\_\+size}{, }\item[{\mbox{\hyperlink{structei__impulse__result__t}{ei\+\_\+impulse\+\_\+result\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{result}{, }\item[{void \texorpdfstring{$\ast$}{*}}]{config\+\_\+ptr}{, }\item[{bool}]{debug}{ = {\ttfamily false}}\end{DoxyParamCaption})}



Do neural network inferencing over the processed feature matrix. 


\begin{DoxyParams}[1]{Parameters}
 & {\em fmatrix} & Processed matrix \texorpdfstring{$>$}{>}\texorpdfstring{$>$}{>} features \mbox{[}array of features\mbox{]} this is input \\
\hline
 & {\em result} & Output classifier results \texorpdfstring{$>$}{>}\texorpdfstring{$>$}{>} output \\
\hline
\mbox{\texttt{ in}}  & {\em debug} & Debug output enable\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The ei impulse error. 
\end{DoxyReturn}
