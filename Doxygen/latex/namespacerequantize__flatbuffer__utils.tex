\doxysection{requantize\+\_\+flatbuffer\+\_\+utils Namespace Reference}
\hypertarget{namespacerequantize__flatbuffer__utils}{}\label{namespacerequantize__flatbuffer__utils}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_a9a2761897d1c77f64f3841a5d58d2aa3}{clip\+\_\+range}} (vals, bit\+\_\+width)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_a1c2346cfe1796ca04dce2fc3cfc4ac23}{quantize\+\_\+data}} (\mbox{\hyperlink{hello__world__model_8cc_ab2a9259b73b53c0bb06a6b242aa7ae32}{data}}, \mbox{\hyperlink{batch__matmul__test_8cc_ab89fe30f475eefcbf59653d919c456bd}{scale}}, \mbox{\hyperlink{batch__matmul__test_8cc_a26c07acd3fa6a6da13ddae2bc5c64b55}{zero\+\_\+point}}=0, bit\+\_\+width=8)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_a6dcdea4167683c4d80a88529ffe69b2d}{dequantize\+\_\+data}} (\mbox{\hyperlink{batch__matmul__test_8cc_a960b49b4072eb4f96ad154be724c49f5}{quantized\+\_\+data}}, \mbox{\hyperlink{batch__matmul__test_8cc_ab89fe30f475eefcbf59653d919c456bd}{scale}}, \mbox{\hyperlink{batch__matmul__test_8cc_a26c07acd3fa6a6da13ddae2bc5c64b55}{zero\+\_\+point}}=0)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_af319647e1813bb96fc223e5f8ee411ac}{change\+\_\+quantization\+\_\+settings\+\_\+8to16}} (tensor, buffers)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_a0ce427fb53c2d4f8f3f32bf4cb00c62c}{change\+\_\+activation\+\_\+tensor\+\_\+8to16}} (tensor, buffers)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_a0cc731ff9a3e83da646a86385b10797a}{requantize\+\_\+bias\+\_\+perlayer}} (buffers, \mbox{\hyperlink{_arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-arduino-examples-main_2src5afb18c45f6346167aa977a363205143_a9cebfb3d6ddd692fbdd268cf4b9ad024}{input}}, weight, bias)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_a82dff25704dcb7aedd82960bf874c83c}{requantize\+\_\+bias\+\_\+perchannel}} (buffers, \mbox{\hyperlink{_arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-arduino-examples-main_2src5afb18c45f6346167aa977a363205143_a9cebfb3d6ddd692fbdd268cf4b9ad024}{input}}, weight, bias)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_a5650c4de90e9d4816234a38421daf886}{set\+\_\+bias\+\_\+type\+\_\+int64}} (buffers, \mbox{\hyperlink{_arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-arduino-examples-main_2src5afb18c45f6346167aa977a363205143_a9cebfb3d6ddd692fbdd268cf4b9ad024}{input}}, weight, bias)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_aa3634fe39cf80e569c92df4929a1b8da}{requantize\+\_\+fully\+\_\+connected}} (tensors, buffers, op)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_a4f8c97c80ef027d3d95437db4a501cf1}{requantize\+\_\+unidirectional\+\_\+sequence\+\_\+lstm}} (tensors, buffers, op)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_aac7d4ab56ee2bc46701c46af21011eec}{requantize\+\_\+softmax}} (tensors, buffers, op)
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_a4734a738b574eac95fc5035c537fe9ec}{requantize\+\_\+transpose\+\_\+conv}} (tensors, buffers, op)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
dict \mbox{\hyperlink{namespacerequantize__flatbuffer__utils_ad62c000900d25f5de9560d35a0888f7e}{TENSOR\+\_\+\+CODE\+\_\+\+TYPE}}
\item 
\mbox{\hyperlink{namespacerequantize__flatbuffer__utils_a32136596a1c56ecf92d926fb3a028a90}{TENSOR\+\_\+\+TYPE\+\_\+\+CODE}} = dict((reversed(item) \mbox{\hyperlink{ei__fill__result__struct_8h_aed2288439daacb76715d34eb9d10e627}{for}} item in TENSOR\+\_\+\+CODE\+\_\+\+TYPE.\+items()))
\end{DoxyCompactItemize}


\doxysubsection{Function Documentation}
\Hypertarget{namespacerequantize__flatbuffer__utils_a0ce427fb53c2d4f8f3f32bf4cb00c62c}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!change\_activation\_tensor\_8to16@{change\_activation\_tensor\_8to16}}
\index{change\_activation\_tensor\_8to16@{change\_activation\_tensor\_8to16}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{change\_activation\_tensor\_8to16()}{change\_activation\_tensor\_8to16()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_a0ce427fb53c2d4f8f3f32bf4cb00c62c} 
requantize\+\_\+flatbuffer\+\_\+utils.\+change\+\_\+activation\+\_\+tensor\+\_\+8to16 (\begin{DoxyParamCaption}\item[{}]{tensor}{, }\item[{}]{buffers}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Change the quantization setting of a activation tensor from int8 to int16\end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_af319647e1813bb96fc223e5f8ee411ac}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!change\_quantization\_settings\_8to16@{change\_quantization\_settings\_8to16}}
\index{change\_quantization\_settings\_8to16@{change\_quantization\_settings\_8to16}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{change\_quantization\_settings\_8to16()}{change\_quantization\_settings\_8to16()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_af319647e1813bb96fc223e5f8ee411ac} 
requantize\+\_\+flatbuffer\+\_\+utils.\+change\+\_\+quantization\+\_\+settings\+\_\+8to16 (\begin{DoxyParamCaption}\item[{}]{tensor}{, }\item[{}]{buffers}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Change the quantization seeting of the tensor from int8 to int16\end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_a9a2761897d1c77f64f3841a5d58d2aa3}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!clip\_range@{clip\_range}}
\index{clip\_range@{clip\_range}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{clip\_range()}{clip\_range()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_a9a2761897d1c77f64f3841a5d58d2aa3} 
requantize\+\_\+flatbuffer\+\_\+utils.\+clip\+\_\+range (\begin{DoxyParamCaption}\item[{}]{vals}{, }\item[{}]{bit\+\_\+width}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Mimic integer calculation.

Clip the range of vals based on bit width.

e.g., clip_range([300], 8) = [127] since int8 have range [-128, 127]

Args:
    vals (np.array): float representation of the integer values
    bit_width (int): number of desired bits for vals

Returns:
    np.array : clipped vals
\end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_a6dcdea4167683c4d80a88529ffe69b2d}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!dequantize\_data@{dequantize\_data}}
\index{dequantize\_data@{dequantize\_data}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{dequantize\_data()}{dequantize\_data()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_a6dcdea4167683c4d80a88529ffe69b2d} 
requantize\+\_\+flatbuffer\+\_\+utils.\+dequantize\+\_\+data (\begin{DoxyParamCaption}\item[{}]{quantized\+\_\+data}{, }\item[{}]{scale}{, }\item[{}]{zero\+\_\+point}{ = {\ttfamily 0}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Dequantize the data to integer type with desired bit width.

Args:
    quantized_data (np.array): quantized data
    scale (float): quantization scale of the data
    zero_point (integer): quantization zero point of the data

Returns:
    np.array : dequantized data
\end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_a1c2346cfe1796ca04dce2fc3cfc4ac23}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!quantize\_data@{quantize\_data}}
\index{quantize\_data@{quantize\_data}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{quantize\_data()}{quantize\_data()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_a1c2346cfe1796ca04dce2fc3cfc4ac23} 
requantize\+\_\+flatbuffer\+\_\+utils.\+quantize\+\_\+data (\begin{DoxyParamCaption}\item[{}]{data}{, }\item[{}]{scale}{, }\item[{}]{zero\+\_\+point}{ = {\ttfamily 0}, }\item[{}]{bit\+\_\+width}{ = {\ttfamily 8}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Quantize the data to integer type with desired bit width.

The quantized data is represented using float since integer calculation in
numpy may differ from other implementations (e.g., no integer saturation
protection in numpy)

Args:
    data (np.array): float data
    scale (float): quantization scale of the data
    zero_point (integer): quantization zero point of the data
    bit_width (int): number of representative bits for vals

Returns:
    np.array : quantized data in float but clipped range
\end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_a82dff25704dcb7aedd82960bf874c83c}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!requantize\_bias\_perchannel@{requantize\_bias\_perchannel}}
\index{requantize\_bias\_perchannel@{requantize\_bias\_perchannel}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{requantize\_bias\_perchannel()}{requantize\_bias\_perchannel()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_a82dff25704dcb7aedd82960bf874c83c} 
requantize\+\_\+flatbuffer\+\_\+utils.\+requantize\+\_\+bias\+\_\+perchannel (\begin{DoxyParamCaption}\item[{}]{buffers}{, }\item[{}]{input}{, }\item[{}]{weight}{, }\item[{}]{bias}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Bias is channel wise quantized. Requantize bias one by one \end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_a0cc731ff9a3e83da646a86385b10797a}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!requantize\_bias\_perlayer@{requantize\_bias\_perlayer}}
\index{requantize\_bias\_perlayer@{requantize\_bias\_perlayer}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{requantize\_bias\_perlayer()}{requantize\_bias\_perlayer()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_a0cc731ff9a3e83da646a86385b10797a} 
requantize\+\_\+flatbuffer\+\_\+utils.\+requantize\+\_\+bias\+\_\+perlayer (\begin{DoxyParamCaption}\item[{}]{buffers}{, }\item[{}]{input}{, }\item[{}]{weight}{, }\item[{}]{bias}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Bias is layer wise quantized \end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_aa3634fe39cf80e569c92df4929a1b8da}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!requantize\_fully\_connected@{requantize\_fully\_connected}}
\index{requantize\_fully\_connected@{requantize\_fully\_connected}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{requantize\_fully\_connected()}{requantize\_fully\_connected()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_aa3634fe39cf80e569c92df4929a1b8da} 
requantize\+\_\+flatbuffer\+\_\+utils.\+requantize\+\_\+fully\+\_\+connected (\begin{DoxyParamCaption}\item[{}]{tensors}{, }\item[{}]{buffers}{, }\item[{}]{op}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Requantize the fully connected op from int8 to int16

Note: CONV_2D and DEPTHWISE_CONV_2D also use this requantize function since they all share the same input/weight/bias configuration. 
See tensorflow/lite/micro/kernels/fully_connected_common.cc
tflite_micro/tensorflow/lite/micro/kernels/depthwise_conv_common.cc
tflite_micro/tensorflow/lite/micro/kernels/conv_common.cc
\end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_aac7d4ab56ee2bc46701c46af21011eec}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!requantize\_softmax@{requantize\_softmax}}
\index{requantize\_softmax@{requantize\_softmax}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{requantize\_softmax()}{requantize\_softmax()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_aac7d4ab56ee2bc46701c46af21011eec} 
requantize\+\_\+flatbuffer\+\_\+utils.\+requantize\+\_\+softmax (\begin{DoxyParamCaption}\item[{}]{tensors}{, }\item[{}]{buffers}{, }\item[{}]{op}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Requantize the softmax op from int8 to int16\end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_a4734a738b574eac95fc5035c537fe9ec}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!requantize\_transpose\_conv@{requantize\_transpose\_conv}}
\index{requantize\_transpose\_conv@{requantize\_transpose\_conv}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{requantize\_transpose\_conv()}{requantize\_transpose\_conv()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_a4734a738b574eac95fc5035c537fe9ec} 
requantize\+\_\+flatbuffer\+\_\+utils.\+requantize\+\_\+transpose\+\_\+conv (\begin{DoxyParamCaption}\item[{}]{tensors}{, }\item[{}]{buffers}{, }\item[{}]{op}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Requantize the transpose conv op from int8 to int16\end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_a4f8c97c80ef027d3d95437db4a501cf1}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!requantize\_unidirectional\_sequence\_lstm@{requantize\_unidirectional\_sequence\_lstm}}
\index{requantize\_unidirectional\_sequence\_lstm@{requantize\_unidirectional\_sequence\_lstm}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{requantize\_unidirectional\_sequence\_lstm()}{requantize\_unidirectional\_sequence\_lstm()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_a4f8c97c80ef027d3d95437db4a501cf1} 
requantize\+\_\+flatbuffer\+\_\+utils.\+requantize\+\_\+unidirectional\+\_\+sequence\+\_\+lstm (\begin{DoxyParamCaption}\item[{}]{tensors}{, }\item[{}]{buffers}{, }\item[{}]{op}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Requantize the unidirectonal sequance lstm op from int8 to int16 \end{DoxyVerb}
 \Hypertarget{namespacerequantize__flatbuffer__utils_a5650c4de90e9d4816234a38421daf886}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!set\_bias\_type\_int64@{set\_bias\_type\_int64}}
\index{set\_bias\_type\_int64@{set\_bias\_type\_int64}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{set\_bias\_type\_int64()}{set\_bias\_type\_int64()}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_a5650c4de90e9d4816234a38421daf886} 
requantize\+\_\+flatbuffer\+\_\+utils.\+set\+\_\+bias\+\_\+type\+\_\+int64 (\begin{DoxyParamCaption}\item[{}]{buffers}{, }\item[{}]{input}{, }\item[{}]{weight}{, }\item[{}]{bias}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Set the bias tensor quantization setting from int32 to int64

Args:
    buffers (list): buffers for the model 
    input (Tensor): the corresponding input tensor for the bias
    weight (Tensor): the corresponding weight tensor for the bias
    bias (Tensor): the bias tensor that need to be modified
\end{DoxyVerb}
 

\doxysubsection{Variable Documentation}
\Hypertarget{namespacerequantize__flatbuffer__utils_ad62c000900d25f5de9560d35a0888f7e}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!TENSOR\_CODE\_TYPE@{TENSOR\_CODE\_TYPE}}
\index{TENSOR\_CODE\_TYPE@{TENSOR\_CODE\_TYPE}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{TENSOR\_CODE\_TYPE}{TENSOR\_CODE\_TYPE}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_ad62c000900d25f5de9560d35a0888f7e} 
dict requantize\+\_\+flatbuffer\+\_\+utils.\+TENSOR\+\_\+\+CODE\+\_\+\+TYPE}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{00001\ =\ \ \{}
\DoxyCodeLine{00002\ \ \ \ \ TensorType.FLOAT32:\ np.float32,}
\DoxyCodeLine{00003\ \ \ \ \ TensorType.FLOAT16:\ np.float16,}
\DoxyCodeLine{00004\ \ \ \ \ TensorType.INT32:\ np.int32,}
\DoxyCodeLine{00005\ \ \ \ \ TensorType.UINT8:\ np.uint8,}
\DoxyCodeLine{00006\ \ \ \ \ TensorType.INT64:\ np.int64,}
\DoxyCodeLine{00007\ \ \ \ \ TensorType.STRING:\ np.string\_,}
\DoxyCodeLine{00008\ \ \ \ \ TensorType.BOOL:\ np.bool\_,}
\DoxyCodeLine{00009\ \ \ \ \ TensorType.INT16:\ np.int16,}
\DoxyCodeLine{00010\ \ \ \ \ TensorType.COMPLEX64:\ np.complex64,}
\DoxyCodeLine{00011\ \ \ \ \ TensorType.INT8:\ np.int8,}
\DoxyCodeLine{00012\ \ \ \ \ TensorType.FLOAT64:\ np.float64,}
\DoxyCodeLine{00013\ \ \ \ \ TensorType.COMPLEX128:\ np.complex128,}
\DoxyCodeLine{00014\ \ \ \ \ TensorType.UINT64:\ np.uint64,}
\DoxyCodeLine{00015\ \ \ \ \ TensorType.RESOURCE:\ \textcolor{stringliteral}{"{}RESOURCE"{}},}
\DoxyCodeLine{00016\ \ \ \ \ TensorType.VARIANT:\ \textcolor{stringliteral}{"{}VARIANT"{}},}
\DoxyCodeLine{00017\ \ \ \ \ TensorType.UINT32:\ np.uint32,}
\DoxyCodeLine{00018\ \ \ \ \ TensorType.UINT16:\ np.uint16,}
\DoxyCodeLine{00019\ \ \ \ \ TensorType.INT4:\ \textcolor{stringliteral}{"{}INT4"{}},}
\DoxyCodeLine{00020\ \}}

\end{DoxyCode}
\Hypertarget{namespacerequantize__flatbuffer__utils_a32136596a1c56ecf92d926fb3a028a90}\index{requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}!TENSOR\_TYPE\_CODE@{TENSOR\_TYPE\_CODE}}
\index{TENSOR\_TYPE\_CODE@{TENSOR\_TYPE\_CODE}!requantize\_flatbuffer\_utils@{requantize\_flatbuffer\_utils}}
\doxysubsubsection{\texorpdfstring{TENSOR\_TYPE\_CODE}{TENSOR\_TYPE\_CODE}}
{\footnotesize\ttfamily \label{namespacerequantize__flatbuffer__utils_a32136596a1c56ecf92d926fb3a028a90} 
requantize\+\_\+flatbuffer\+\_\+utils.\+TENSOR\+\_\+\+TYPE\+\_\+\+CODE = dict((reversed(item) \mbox{\hyperlink{ei__fill__result__struct_8h_aed2288439daacb76715d34eb9d10e627}{for}} item in TENSOR\+\_\+\+CODE\+\_\+\+TYPE.\+items()))}

