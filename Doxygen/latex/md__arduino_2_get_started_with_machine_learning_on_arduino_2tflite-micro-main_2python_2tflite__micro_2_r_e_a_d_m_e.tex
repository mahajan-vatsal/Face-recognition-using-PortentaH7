\chapter{The \texorpdfstring{$<$}{<}tt\texorpdfstring{$>$}{>}tflite\+\_\+micro\texorpdfstring{$<$}{<}/tt\texorpdfstring{$>$}{>} Python Package}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e}{}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e}\index{The $<$tt$>$tflite\_micro$<$/tt$>$ Python Package@{The $<$tt$>$tflite\_micro$<$/tt$>$ Python Package}}
\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md113}%
\Hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md113}%


This directory contains the {\ttfamily \doxylink{namespacetflite__micro}{tflite\+\_\+micro}} Python package. The following is mainly documentation for its developers.

The {\ttfamily \doxylink{namespacetflite__micro}{tflite\+\_\+micro}} package contains a complete TFLM interpreter built as a CPython extension module. The build of simple Python packages may be driven by standard Python package builders such as {\ttfamily build}, {\ttfamily setuptools}, and {\ttfamily flit}; however, as TFLM is first and foremost a large C/\+C++ project, {\ttfamily \doxylink{namespacetflite__micro}{tflite\+\_\+micro}}\textquotesingle{}s build is instead driven by its C/\+C++ build system Bazel.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md114}{}\doxysection{\texorpdfstring{Building and installing locally}{Building and installing locally}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md114}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md115}{}\doxysubsection{\texorpdfstring{Building}{Building}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md115}
The Bazel target {\ttfamily //python/tflite\+\_\+micro\+:whl.\+dist} builds a {\ttfamily \doxylink{namespacetflite__micro}{tflite\+\_\+micro}} Python {\itshape .whl} under the output directory {\ttfamily bazel-\/bin/python/tflite\+\_\+micro/whl\+\_\+dist}. For example\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{\%\ bazel\ build\ //python/tflite\_micro:whl.dist}
\DoxyCodeLine{....}
\DoxyCodeLine{Target\ //python/tflite\_micro:whl.dist\ up-\/to-\/date:}
\DoxyCodeLine{\ \ bazel-\/bin/python/tflite\_micro/whl\_dist}
\DoxyCodeLine{}
\DoxyCodeLine{\%\ tree\ bazel-\/bin/python/tflite\_micro/whl\_dist}
\DoxyCodeLine{bazel-\/bin/python/tflite\_micro/whl\_dist}
\DoxyCodeLine{└──\ tflite\_micro-\/0.dev20230920161638-\/py3-\/none-\/any.whl}

\end{DoxyCode}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md116}{}\doxysubsection{\texorpdfstring{Installing}{Installing}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md116}
Install the resulting {\itshape .whl} via pip. For example, in a Python virtual environment\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{\%\ python3\ -\/m\ venv\ \string~/tmp/venv}
\DoxyCodeLine{\%\ source\ \string~/tmp/venv/bin/activate}
\DoxyCodeLine{(venv)\ \$\ pip\ install\ bazel-\/bin/python/tflite\_micro/whl\_dist/tflite\_micro-\/0.dev20230920161638-\/py3-\/none-\/any.whl}
\DoxyCodeLine{Processing\ ./bazel-\/bin/python/tflite\_micro/whl\_dist/tflite\_micro-\/0.dev20230920161638-\/py3-\/none-\/any.whl}
\DoxyCodeLine{....}
\DoxyCodeLine{Installing\ collected\ packages:\ [....]}

\end{DoxyCode}


The package should now be importable and usable. For example\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{(venv)\ \$\ python}
\DoxyCodeLine{Python\ 3.10.12\ (main,\ Jun\ 11\ 2023,\ 05:26:28)\ [GCC\ 11.4.0]\ on\ linux}
\DoxyCodeLine{Type\ "{}help"{},\ "{}copyright"{},\ "{}credits"{}\ or\ "{}license"{}\ for\ more\ information.}
\DoxyCodeLine{>>>\ import\ tflite\_micro}
\DoxyCodeLine{>>>\ tflite\_micro.postinstall\_check.passed()}
\DoxyCodeLine{True}
\DoxyCodeLine{>>>\ \ i\ =\ tflite\_micro.runtime.Interpreter.from\_file("{}foo.tflite"{})}
\DoxyCodeLine{>>>\ \#\ etc.}

\end{DoxyCode}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md117}{}\doxysection{\texorpdfstring{Building and uploading to Py\+PI}{Building and uploading to Py\+PI}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md117}
The {\itshape .whl} generated above is unsuitable for distribution to the wider world via Py\+PI. The extension module is inevitably compiled against a particular Python implementation and platform C library. The resulting package is only binary-\/compatible with a system running the same Python implementation and a compatible (typically the same or newer) C library.

The solution is to distribute multiple \texorpdfstring{$\ast$}{*}.whl\texorpdfstring{$\ast$}{*}s, one built for each Python implementation and platform combination. TFLM accomplishes this by running Bazel builds from within multiple, uniquely configured Docker containers. The images used are based on standards-\/conforming images published by the Python Package Authority (Py\+PA) for exactly such use.

Python \texorpdfstring{$\ast$}{*}.whl\texorpdfstring{$\ast$}{*}s contain metadata used by installers such as {\ttfamily pip} to determine which distributions (\texorpdfstring{$\ast$}{*}.whl\texorpdfstring{$\ast$}{*}s) are compatible with the target platform. See the Py\+PA specification for \href{https://packaging.python.org/en/latest/specifications/platform-compatibility-tags/}{\texttt{ platform compatibility tags}}.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md118}{}\doxysubsection{\texorpdfstring{Building}{Building}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md118}
In an environment with a working Docker installation, run the script {\ttfamily python/tflite\+\_\+micro/pypi\+\_\+build.\+sh \texorpdfstring{$<$}{<}python-\/tag\texorpdfstring{$>$}{>}} once for each tag. The script\textquotesingle{}s online help ({\ttfamily -\/-\/help}) lists the available tags. The script builds an appropriate Docker container and invokes a Bazel build and test within it. For example\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{\%\ python/tflite\_micro/pypi\_build.sh\ cp310}
\DoxyCodeLine{[+]\ Building\ 2.6s\ (7/7)\ FINISHED}
\DoxyCodeLine{=>\ writing\ image\ sha256:900704dad7fa27938dcc1c5057c0e760fb4ab0dff676415182455ae66546bbd4}
\DoxyCodeLine{bazel\ build\ //python/tflite\_micro:whl.dist\ \(\backslash\)}
\DoxyCodeLine{\ \ \ \ -\/-\///python/tflite\_micro:compatibility\_tag=cp310\_cp310\_manylinux\_2\_28\_x86\_64}
\DoxyCodeLine{bazel\ test\ //python/tflite\_micro:whl\_test\ \(\backslash\)}
\DoxyCodeLine{\ \ \ \ -\/-\///python/tflite\_micro:compatibility\_tag=cp310\_cp310\_manylinux\_2\_28\_x86\_64}
\DoxyCodeLine{//python/tflite\_micro:whl\_test}
\DoxyCodeLine{Executed\ 1\ out\ of\ 1\ test:\ 1\ test\ passes.}
\DoxyCodeLine{Output:}
\DoxyCodeLine{bazel-\/pypi-\/out/tflite\_micro-\/0.dev20230920031310-\/cp310-\/cp310-\/manylinux\_2\_28\_x86\_64.whl}

\end{DoxyCode}


By default, \texorpdfstring{$\ast$}{*}.whl\texorpdfstring{$\ast$}{*}s are generated under the output directory {\ttfamily bazel-\/pypi-\/out/}.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md119}{}\doxysubsection{\texorpdfstring{Uploading to Py\+PI}{Uploading to Py\+PI}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md119}
Upload the generated \texorpdfstring{$\ast$}{*}.whl\texorpdfstring{$\ast$}{*}s to Py\+PI with the script {\ttfamily python/tflite\+\_\+micro/pypi\+\_\+upload.\+sh}. This script lightly wraps the standard upload tool {\ttfamily twine}. A Py\+PI authentication token must be assigned to {\ttfamily TWINE\+\_\+\+PASSWORD} in the environment. For example\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{\%\ export\ TWINE\_PASSWORD=pypi-\/AgENdGV[....]}
\DoxyCodeLine{\%\ ./python/tflite\_micro/pypi\_upload.sh\ -\/-\/test-\/pypi\ bazel-\/pypi-\/out/tflite\_micro-\/*.whl}
\DoxyCodeLine{Uploading\ distributions\ to\ https://test.pypi.org/legacy/}
\DoxyCodeLine{Uploading\ tflite\_micro-\/0.dev20230920031310-\/cp310-\/cp310-\/manylinux\_2\_28\_x86\_64.whl}
\DoxyCodeLine{Uploading\ tflite\_micro-\/0.dev20230920031310-\/cp311-\/cp311-\/manylinux\_2\_28\_x86\_64.whl}
\DoxyCodeLine{View\ at:}
\DoxyCodeLine{https://test.pypi.org/project/tflite-\/micro/0.dev20230920031310/}

\end{DoxyCode}


See the script\textquotesingle{}s online help ({\ttfamily -\/-\/help}) for more.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md120}{}\doxysection{\texorpdfstring{Using {\ttfamily tflite\+\_\+micro} from within the TFLM source tree}{Using {\ttfamily tflite\+\_\+micro} from within the TFLM source tree}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md120}
\+:construction\+: {\itshape The remainder of this document is under construction and may contain some obsolete information.} \+:construction\+:

The only package that needs to be included in the {\ttfamily \doxylink{namespace_b_u_i_l_d}{BUILD}} file is {\ttfamily //python/tflite\+\_\+micro\+:runtime}. It contains all the correct dependencies to build the Python interpreter.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md121}{}\doxysubsection{\texorpdfstring{Examples}{Examples}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md121}
Depending on the workflow, the package import path may be slightly different.

A simple end-\/to-\/end example is the test {\ttfamily \doxylink{runtime__test_8py}{python/tflite\+\_\+micro/runtime\+\_\+test.\+py}\+:test\+Compare\+With\+TFLite()}. It shows how to compare inference results between TFLite and TFLM.

A basic usage of the TFLM Python interpreter looks like the following. The input to the Python interpreter should be a converted TFLite flatbuffer in either bytearray format or file format.


\begin{DoxyCode}{0}
\DoxyCodeLine{\#\ For\ the\ Bazel\ workflow}
\DoxyCodeLine{from\ tflite\_micro.python.tflite\_micro\ import\ runtime}
\DoxyCodeLine{}
\DoxyCodeLine{}
\DoxyCodeLine{\#\ If\ model\ is\ a\ bytearray}
\DoxyCodeLine{tflm\_interpreter\ =\ runtime.Interpreter.from\_bytes(model\_data)}
\DoxyCodeLine{\#\ If\ model\ is\ a\ file}
\DoxyCodeLine{tflm\_interpreter\ =\ runtime.Interpreter.from\_file(model\_filepath)}
\DoxyCodeLine{}
\DoxyCodeLine{\#\ Run\ inference\ on\ TFLM\ using\ an\ ndarray\ \`{}data\_x`}
\DoxyCodeLine{tflm\_interpreter.set\_input(data\_x,\ 0)}
\DoxyCodeLine{tflm\_interpreter.invoke()}
\DoxyCodeLine{tflm\_output\ =\ tflm\_interpreter.get\_output(0)}

\end{DoxyCode}


Input and output tensor details can also be queried using the Python API\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{print(tflm\_interpreter.get\_input\_details(0))}
\DoxyCodeLine{print(tflm\_interpreter.get\_output\_details(0))}

\end{DoxyCode}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md122}{}\doxysubsection{\texorpdfstring{Technical Details}{Technical Details}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md122}
The Python interpreter uses \href{https://github.com/pybind/pybind11}{\texttt{ pybind11}} to expose an evolving set of C++ APIs. The Bazel build leverages the \href{https://github.com/pybind/pybind11_bazel}{\texttt{ pybind11\+\_\+bazel extension}}.

The most updated Python APIs can be found in {\ttfamily \doxylink{python_2tflite__micro_2runtime_8py}{python/tflite\+\_\+micro/runtime.\+py}}.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md123}{}\doxysubsection{\texorpdfstring{Custom Ops}{Custom Ops}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md123}
The Python interpreter works with models with \href{https://www.tensorflow.org/lite/guide/ops_custom}{\texttt{ custom ops}} but special steps need to be taken to make sure that it can retrieve the right implementation. This is currently compatible with the Bazel workflow only.


\begin{DoxyEnumerate}
\item Implement the custom op in C++
\end{DoxyEnumerate}

Assuming that the custom is already implemented according to the linked guide,


\begin{DoxyCode}{0}
\DoxyCodeLine{//\ custom\_op.cc}
\DoxyCodeLine{TfLiteRegistration\ *Register\_YOUR\_CUSTOM\_OP()\ \{}
\DoxyCodeLine{\ \ \ \ //\ Do\ custom\ op\ stuff}
\DoxyCodeLine{\}}
\DoxyCodeLine{}
\DoxyCodeLine{//\ custom\_op.h}
\DoxyCodeLine{TfLiteRegistration\ *Register\_YOUR\_CUSTOM\_OP();}

\end{DoxyCode}



\begin{DoxyEnumerate}
\item Implement a custom op Registerer
\end{DoxyEnumerate}

A Registerer of the following signature is required to wrap the custom op and add it to TFLM\textquotesingle{}s ops resolver. For example,


\begin{DoxyCode}{0}
\DoxyCodeLine{\#include\ "{}custom\_op.h"{}}
\DoxyCodeLine{\#include\ "{}tensorflow/lite/micro/all\_ops\_resolver.h"{}}
\DoxyCodeLine{}
\DoxyCodeLine{namespace\ tflite\ \{}
\DoxyCodeLine{}
\DoxyCodeLine{extern\ "{}C"{}\ bool\ SomeCustomRegisterer(tflite::PythonOpsResolver*\ resolver)\ \{}
\DoxyCodeLine{\ \ \ \ TfLiteStatus\ status\ =\ resolver-\/>AddCustom("{}CustomOp"{},\ tflite::Register\_YOUR\_CUSTOM\_OP());}
\DoxyCodeLine{\ \ \ \ if\ (status\ !=\ kTfLiteOk)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ return\ false;}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\ \ \ \ return\ true;}
\DoxyCodeLine{\}}

\end{DoxyCode}



\begin{DoxyEnumerate}
\item Include the implementation of custom op and registerer in the caller\textquotesingle{}s build
\end{DoxyEnumerate}

For the Bazel workflow, it\textquotesingle{}s recommended to create a package that includes the custom op\textquotesingle{}s and the registerer\textquotesingle{}s implementation, because it needs to be included in the target that calls the Python interpreter with custom ops.


\begin{DoxyEnumerate}
\item Pass the registerer into the Python interpreter during instantiation
\end{DoxyEnumerate}

For example,


\begin{DoxyCode}{0}
\DoxyCodeLine{interpreter\ =\ runtime.Interpreter.from\_file(}
\DoxyCodeLine{\ \ \ \ model\_path=model\_path,}
\DoxyCodeLine{\ \ \ \ custom\_op\_registerers=['SomeCustomRegisterer'])}

\end{DoxyCode}


The interpreter will then perform a dynamic lookup for the symbol called {\ttfamily Some\+Custom\+Registerer()} and call it. This ensures that the custom op is properly included in TFLM\textquotesingle{}s op resolver. This approach is very similar to TFLite\textquotesingle{}s custom op support.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md124}{}\doxysubsection{\texorpdfstring{Print Allocations}{Print Allocations}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2python_2tflite__micro_2_r_e_a_d_m_e_autotoc_md124}
The Python interpreter can also be used to print memory arena allocations. This is very helpful to figure out actual memory arena usage.

For example,


\begin{DoxyCode}{0}
\DoxyCodeLine{tflm\_interpreter.print\_allocations()}

\end{DoxyCode}


will print


\begin{DoxyCode}{0}
\DoxyCodeLine{[RecordingMicroAllocator]\ Arena\ allocation\ total\ 10016\ bytes}
\DoxyCodeLine{[RecordingMicroAllocator]\ Arena\ allocation\ head\ 7744\ bytes}
\DoxyCodeLine{[RecordingMicroAllocator]\ Arena\ allocation\ tail\ 2272\ bytes}
\DoxyCodeLine{[RecordingMicroAllocator]\ 'TfLiteEvalTensor\ data'\ used\ 312\ bytes\ with\ alignment\ overhead\ (requested\ 312\ bytes\ for\ 13\ allocations)}
\DoxyCodeLine{[RecordingMicroAllocator]\ 'Persistent\ TfLiteTensor\ data'\ used\ 224\ bytes\ with\ alignment\ overhead\ (requested\ 224\ bytes\ for\ 2\ tensors)}
\DoxyCodeLine{[RecordingMicroAllocator]\ 'Persistent\ TfLiteTensor\ quantization\ data'\ used\ 64\ bytes\ with\ alignment\ overhead\ (requested\ 64\ bytes\ for\ 4\ allocations)}
\DoxyCodeLine{[RecordingMicroAllocator]\ 'Persistent\ buffer\ data'\ used\ 640\ bytes\ with\ alignment\ overhead\ (requested\ 608\ bytes\ for\ 10\ allocations)}
\DoxyCodeLine{[RecordingMicroAllocator]\ 'NodeAndRegistration\ struct'\ used\ 440\ bytes\ with\ alignment\ overhead\ (requested\ 440\ bytes\ for\ 5\ NodeAndRegistration\ structs)}

\end{DoxyCode}


10016 bytes is the actual memory arena size.

During instantiation via the class methods {\ttfamily runtime.\+Interpreter.\+from\+\_\+file} or {\ttfamily runtime.\+Interpreter.\+from\+\_\+bytes}, if {\ttfamily arena\+\_\+size} is not explicitly specified, the interpreter will default to a heuristic which is 10x the model size. This can be adjusted manually if desired. 