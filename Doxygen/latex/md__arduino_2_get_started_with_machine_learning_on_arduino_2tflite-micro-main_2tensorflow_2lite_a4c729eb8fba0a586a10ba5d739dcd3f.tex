\chapter{002\+\_\+16x8\+\_\+quantization\+\_\+port}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f}{}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f}\index{002\_16x8\_quantization\_port@{002\_16x8\_quantization\_port}}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md241}{}\doxysection{\texorpdfstring{Tensor\+Flow Lite for Microcontrollers Port of 16x8 Quantized Operators}{Tensor\+Flow Lite for Microcontrollers Port of 16x8 Quantized Operators}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md241}
\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{2}{|X[-1]}|}
\hline
\cellcolor{\tableheadbgcolor}\textbf{ Status   }&\cellcolor{\tableheadbgcolor}\textbf{ Proposed    }\\\cline{1-2}
\endfirsthead
\hline
\endfoot
\hline
\cellcolor{\tableheadbgcolor}\textbf{ Status   }&\cellcolor{\tableheadbgcolor}\textbf{ Proposed    }\\\cline{1-2}
\endhead
{\bfseries{RFC \#2}}   &\href{https://github.com/tensorflow/tensorflow/pull/46767}{\texttt{ 46767}}    \\\cline{1-2}
{\bfseries{Author(s)}}   &Daniel Situnayake (\href{mailto:dan@edgeimpulse.com}{\texttt{ dan@edgeimpulse.\+com}})    \\\cline{1-2}
{\bfseries{Sponsor}}   &Pete Warden (\href{mailto:petewarden@google.com}{\texttt{ petewarden@google.\+com}})    \\\cline{1-2}
{\bfseries{Updated}}   &2021-\/01-\/28   \\\cline{1-2}
\end{longtabu}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md242}{}\doxysubsection{\texorpdfstring{Objective}{Objective}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md242}
Tensor\+Flow Lite has kernel implementations that support 8 bit quantized weights but use 16 bit activations. We wish to port these implementations to Tensor\+Flow Lite for Microcontrollers. The increased precision available for activations can improve performance for some quantized models.

Arm have agreed to support the initiative by adding the necessary 16x8 APIs to CMSIS-\/\+NN and porting the CMSIS-\/\+NN kernels.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md243}{}\doxysubsubsection{\texorpdfstring{Goals}{Goals}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md243}

\begin{DoxyItemize}
\item Port a subset of 16x8 reference kernels from Tensor\+Flow Lite to Tensor\+Flow Lite Micro
\item Avoid increasing default code size or arena size of Tensor\+Flow Lite Micro
\item Lay the groundwork for creating a CMSIS-\/\+NN port of the 16x8 kernels
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md244}{}\doxysubsubsection{\texorpdfstring{Non-\/goals}{Non-\/goals}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md244}

\begin{DoxyItemize}
\item Port every single operator to 16x8; we only plan to port a subset of those with existing reference implementations
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md245}{}\doxysubsection{\texorpdfstring{Motivation}{Motivation}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md245}
Some networks that suffer unacceptable degradation when quantized with 8 bit weights and 8 bit activations perform adequately when quantized with 8 bit weights and 16 bit activations. The \href{https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8}{\texttt{ Tensor\+Flow Lite documentation}} states the following\+:

\begin{quote}
\mbox{[}16x8 quantization\mbox{]} mode can improve accuracy of the quantized model significantly, when activations are sensitive to the quantization, while still achieving almost 3-\/4x reduction in model size. Moreover, this fully quantized model can be consumed by integer-\/only hardware accelerators. \end{quote}
Edge Impulse, a company that deploys Tensor\+Flow Lite for Microcontrollers as part of its embedded machine learning pipeline, has gathered feedback from customers with production models for which 8 bit quantization results in unacceptable degradation but for whom 16x8 is fine.

While 16x8 quantization is well supported within Tensor\+Flow Lite, it is not currently supported within Tensor\+Flow Lite for Microcontrollers. Porting the Tensor\+Flow Lite reference kernels is relatively straightforward and will improve adoption of Tensor\+Flow Lite for Microcontrollers with users for whom degradation is too severe with full 8 bit quantization.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md246}{}\doxysubsection{\texorpdfstring{User Benefit}{User Benefit}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md246}
The headline would be "{}16x8 kernels improve accuracy for quantized models on microcontrollers without increasing model size"{}.

Users would benefit in the following ways\+:


\begin{DoxyItemize}
\item Improved accuracy for quantized models without increasing model size (in exchange for additional runtime memory usage)
\item Improved performance under certain conditions (for example, 16x8 CMSIS-\/\+NN kernels will run faster) than 8 bit kernels since less unpacking is required)
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md247}{}\doxysubsection{\texorpdfstring{Design Proposal}{Design Proposal}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md247}
We propose that the 16x8 kernels are ported from the Tensor\+Flow Lite reference kernels to Tensor\+Flow Lite for Microcontrollers following the process in the \href{https://docs.google.com/document/d/1KLJTPWm4TUKB9YyIqFJl9VCP0ZMJDt_P8RNpRmwqMxw/edit\#heading=h.5x0d5h95i329}{\texttt{ Porting Tensor\+Flow Lite Ops to Micro}} guide.

We wish to ensure that the following kernels are compatible with 16x8 mode\+:


\begin{DoxyItemize}
\item Conv2D
\item Max\+Pool2D
\item Depthwise\+Conv2D
\item Fully\+Connected
\item Relu
\item Relu6
\item Tanh
\item Softmax
\item Pad
\item Reshape
\item Pack
\item Unpack
\item Add
\item Mul
\end{DoxyItemize}

Adding the 16x8 kernels directly to TFLM alongside the existing kernels would increase the default code size by an unacceptable amount. Instead, we will make use of the kernel registration API currently under development by the TFLM team. The use of this is demonstrated in the \href{https://github.com/tensorflow/tensorflow/blob/a30d20b632b4ffbfd437ccf8ee205fef0917a3eb/tensorflow/lite/micro/benchmarks/keyword_benchmark.cc\#L56}{\texttt{ Keyword benchmark code}}. By doing this, the end user can decide which kernels and dependencies they want to include (e.\+g. 8 bit, 16x8, or float32).

For example, the following could be registered\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{//\ Support\ for\ all\ datatypes}
\DoxyCodeLine{op\_resolver-\/>AddFullyConnected(tflite::Register\_FULLY\_CONNECTED);}
\DoxyCodeLine{//\ Support\ for\ 8\ bit\ quantized\ models}
\DoxyCodeLine{op\_resolver-\/>AddFullyConnected(tflite::Register\_FULLY\_CONNECTED\_INT8);}
\DoxyCodeLine{//\ Support\ for\ 16x8\ quantized\ models}
\DoxyCodeLine{op\_resolver-\/>AddFullyConnected(tflite::Register\_FULLY\_CONNECTED\_INT16X8());}

\end{DoxyCode}


This means that kernels not currently using this registration API will need to be refactored to use it. Currently only {\bfseries{Fully\+Connected}} uses the API.

The following associated tasks will be required to support this work\+:


\begin{DoxyItemize}
\item Build or port unit tests for the new kernels
\item Prove that code memory is not impacted by running benchmarks before and after the port
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md248}{}\doxysubsubsection{\texorpdfstring{Alternatives Considered}{Alternatives Considered}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md248}

\begin{DoxyItemize}
\item An alternative would be to add the 16x8 kernels without using the new kernel registration API, but this would result in a major increase in code size.
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md249}{}\doxysubsubsection{\texorpdfstring{Performance Implications}{Performance Implications}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md249}

\begin{DoxyItemize}
\item Impact on memory usage for current modes (int8 and float32) will be minimal. This will be confirmed by benchmarking of current performance against performance of the submitted changes.
\item When 16x8 mode is used, RAM usage will be approximately 2x. Latency may change depending on the target platform.
\item End to end and unit tests will be updated to prove that the new implementations are operating correctly.
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md250}{}\doxysubsubsection{\texorpdfstring{Dependencies}{Dependencies}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md250}

\begin{DoxyItemize}
\item No additional dependencies will be added to Tensor\+Flow
\item No other parts of Tensor\+Flow will be affected
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md251}{}\doxysubsubsection{\texorpdfstring{Engineering Impact}{Engineering Impact}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md251}

\begin{DoxyItemize}
\item Impact on binary size should be minimal
\item Test times may increase due to additional kernel unit tests
\item The reference kernels already exist within Tensor\+Flow Lite so there will be minimal additional maintenance
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md252}{}\doxysubsubsection{\texorpdfstring{Platforms and Environments}{Platforms and Environments}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md252}

\begin{DoxyItemize}
\item The proposed changes will work on all currently supported platforms
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md253}{}\doxysubsubsection{\texorpdfstring{Best Practices}{Best Practices}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md253}

\begin{DoxyItemize}
\item Tensor\+Flow Lite for Microcontrollers should be updated to indicate that 16x8 kernels are now available
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md254}{}\doxysubsubsection{\texorpdfstring{Tutorials and Examples}{Tutorials and Examples}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md254}

\begin{DoxyItemize}
\item A benchmark will be added to \href{https://github.com/tensorflow/tensorflow/tree/975335bc83bf3cb80a71a04ed407725508709808/tensorflow/lite/micro/benchmarks}{\texttt{ {\ttfamily tensorflow/lite/micro/benchmarks}}} that demonstrates the use of the ops that provide a 16x8 kernel.
\item A Colab will be created that demonstrates quantizing a model in 16x8 mode and exporting it as a C header file for use with Tensor\+Flow Lite for Microcontrollers
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md255}{}\doxysubsubsection{\texorpdfstring{Compatibility}{Compatibility}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md255}

\begin{DoxyItemize}
\item This work will improve compatibility and feature parity between Tensor\+Flow Lite and Tensor\+Flow Lite for Microcontrollers
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md256}{}\doxysubsubsection{\texorpdfstring{User Impact}{User Impact}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md256}

\begin{DoxyItemize}
\item Since TFLM does not have a versioning system the feature can be rolled out as any other commit
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md257}{}\doxysubsection{\texorpdfstring{Implementation plan}{Implementation plan}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md257}
The work will be broken down into a series of pull requests, some for the benchmarks and some for each kernel.

Benchmark pull requests\+:
\begin{DoxyItemize}
\item PR1\+: Create a new benchmark in \href{https://github.com/tensorflow/tensorflow/tree/975335bc83bf3cb80a71a04ed407725508709808/tensorflow/lite/micro/benchmarks}{\texttt{ {\ttfamily tensorflow/lite/micro/benchmarks}}} that attempts to run a 16x8 model that includes the kernels mentioned in this RFC. The modelâ€™s weights and biases can be random. The benchmark should use the Micro\+Mutable\+Op\+Resolver. The PR should include the Colab used to generate the model.
\item PR2\+: Port the person\+\_\+detection and keyword benchmarks to use the Micro\+Mutable\+Op\+Resolver.
\item PR3\+: Add code to both benchmarks that prints the arena size using the \href{https://github.com/tensorflow/tensorflow/blob/ee87d58a6504375c28f21ea303f0eefa29118c38/tensorflow/lite/micro/docs/memory_management.md\#recording-memory-apis}{\texttt{ {\ttfamily Recording\+Memory\+Allocator}}}.
\end{DoxyItemize}

For each kernel\+:
\begin{DoxyItemize}
\item PR1\+: Refactor the implementation to support the new kernel variant registration API.
\item PR2\+: Add 16x8 support and make sure that the benchmark binary and arena sizes are unchanged.
\end{DoxyItemize}

Note that @njeffrie from the TF Lite Micro team also plans to prepare PR(s) for the kernels that are of interest internally (without using the kernel variant registation API for binary size). This will provide some quick examples of porting the kernels.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md258}{}\doxysubsection{\texorpdfstring{Questions and Discussion Topics}{Questions and Discussion Topics}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_a4c729eb8fba0a586a10ba5d739dcd3f_autotoc_md258}
