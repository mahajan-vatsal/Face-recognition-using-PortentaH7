\chapter{README}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474}{}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474}\index{README@{README}}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md259}{}\doxysection{\texorpdfstring{Hello World Example}{Hello World Example}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md259}
This example is designed to demonstrate the absolute basics of using \href{https://www.tensorflow.org/lite/microcontrollers}{\texttt{ Tensor\+Flow Lite for Microcontrollers}}. It includes the full end-\/to-\/end workflow of training a model, converting it for use with Tensor\+Flow Lite for Microcontrollers for running inference on a microcontroller.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md260}{}\doxysubsection{\texorpdfstring{Table of contents}{Table of contents}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md260}

\begin{DoxyItemize}
\item Run the evaluate.py script on a development machine
\item Run the tests on a development machine
\item Train your own model
\end{DoxyItemize}\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md261}{}\doxysubsection{\texorpdfstring{Run the evaluate.\+py script on a development machine}{Run the evaluate.\+py script on a development machine}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md261}
The evaluate.\+py script runs the hello\+\_\+world.\+tflite model with x\+\_\+values in the range of \mbox{[}0, 2\texorpdfstring{$\ast$}{*}\+PI\mbox{]}. The script plots a diagram of the predicted value of sinwave using TFLM interpreter and compare that prediction with the actual value generated by the numpy lib. 
\begin{DoxyCode}{0}
\DoxyCodeLine{bazel\ build\ tensorflow/lite/micro/examples/hello\_world:evaluate}
\DoxyCodeLine{bazel\ run\ tensorflow/lite/micro/examples/hello\_world:evaluate}
\DoxyCodeLine{bazel\ run\ tensorflow/lite/micro/examples/hello\_world:evaluate\ -\/-\/\ -\/-\/use\_tflite}

\end{DoxyCode}
  \hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md262}{}\doxysubsection{\texorpdfstring{Run the evaluate\+\_\+test.\+py script on a development machine}{Run the evaluate\+\_\+test.\+py script on a development machine}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md262}
These tests verify the input/output as well as the prediction of the hello\+\_\+world.\+tflite model. There is a test to also verify the correctness of the model by running both TFLM and TFlite interpreter and then comparing the prediction from both interpreters. 
\begin{DoxyCode}{0}
\DoxyCodeLine{bazel\ build\ tensorflow/lite/micro/examples/hello\_world:evaluate\_test}
\DoxyCodeLine{bazel\ run\ tensorflow/lite/micro/examples/hello\_world:evaluate\_test}

\end{DoxyCode}
\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md263}{}\doxysubsection{\texorpdfstring{Run the tests on a development machine}{Run the tests on a development machine}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md263}
Run the cc test using bazel 
\begin{DoxyCode}{0}
\DoxyCodeLine{bazel\ run\ tensorflow/lite/micro/examples/hello\_world:hello\_world\_test}

\end{DoxyCode}
 And to run it using make 
\begin{DoxyCode}{0}
\DoxyCodeLine{make\ -\/f\ tensorflow/lite/micro/tools/make/Makefile\ test\_hello\_world\_test}

\end{DoxyCode}


The source for the test is \href{hello_world_test.cc}{\texttt{ hello\+\_\+world\+\_\+test.\+cc}}. It\textquotesingle{}s a fairly small amount of code that creates an interpreter, gets a handle to a model that\textquotesingle{}s been compiled into the program, and then invokes the interpreter with the model and sample inputs.\hypertarget{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md264}{}\doxysubsection{\texorpdfstring{Train your own model}{Train your own model}}\label{md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_968229d555ede94460235726ac1c7474_autotoc_md264}
So far you have used an existing trained model to run inference on microcontrollers. If you wish to train your own model, here are the scripts that can help you to achieve that.


\begin{DoxyCode}{0}
\DoxyCodeLine{bazel\ build\ tensorflow/lite/micro/examples/hello\_world:train}

\end{DoxyCode}
 And to run it 
\begin{DoxyCode}{0}
\DoxyCodeLine{bazel-\/bin/tensorflow/lite/micro/examples/hello\_world/train\ -\/-\/save\_tf\_model\ }
\DoxyCodeLine{-\/-\/save\_dir=/tmp/model\_created/}

\end{DoxyCode}
 The above script will create a TF model and TFlite model inside the {\ttfamily /tmp/model\+\_\+created} directory.

Now the above model is a {\ttfamily float} model. Means it can take floating point input and can produce floating point output.

If we want a fully quantized model we can use the {\ttfamily \doxylink{ptq_8py}{ptq.\+py}} script inside the quantization directory. The {\ttfamily \doxylink{ptq_8py}{ptq.\+py}} script can take a floating point TF model and can produce a quantized model.

Build the {\ttfamily \doxylink{ptq_8py}{ptq.\+py}} script like 
\begin{DoxyCode}{0}
\DoxyCodeLine{bazel\ build\ tensorflow/lite/micro/examples/hello\_world/quantization:ptq}

\end{DoxyCode}


Then we can run the {\ttfamily ptq} script to convert the float model to quant model as follows. Note that we are using the directory ({\ttfamily /tmp/model\+\_\+created}) of the TF model as the source\+\_\+model\+\_\+dir here. The quant model (named {\ttfamily hello\+\_\+world\+\_\+int8.\+tflite}) will be created inside the target\+\_\+dir. The {\ttfamily \doxylink{ptq_8py}{ptq.\+py}} script will convert the {\ttfamily TF model} found inside the {\ttfamily /tmp/model\+\_\+created} folder and convert it to a {\ttfamily int8} TFlite model. 
\begin{DoxyCode}{0}
\DoxyCodeLine{bazel-\/bin/tensorflow/lite/micro/examples/hello\_world/quantization/ptq\ \ }
\DoxyCodeLine{-\/-\/source\_model\_dir=/tmp/model\_created\ -\/-\/target\_dir=/tmp/quant\_model/}

\end{DoxyCode}
 