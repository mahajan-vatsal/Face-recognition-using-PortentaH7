\doxysection{Face\+\_\+\+Access\+\_\+inferencing/src/edge-\/impulse-\/sdk/classifier/inferencing\+\_\+engines/tensorrt.h File Reference}
\hypertarget{tensorrt_8h}{}\label{tensorrt_8h}\index{Face\_Access\_inferencing/src/edge-\/impulse-\/sdk/classifier/inferencing\_engines/tensorrt.h@{Face\_Access\_inferencing/src/edge-\/impulse-\/sdk/classifier/inferencing\_engines/tensorrt.h}}
{\ttfamily \#include "{}model-\/parameters/model\+\_\+metadata.\+h"{}}\newline
{\ttfamily \#include "{}edge-\/impulse-\/sdk/porting/ei\+\_\+classifier\+\_\+porting.\+h"{}}\newline
{\ttfamily \#include "{}edge-\/impulse-\/sdk/classifier/ei\+\_\+fill\+\_\+result\+\_\+struct.\+h"{}}\newline
{\ttfamily \#include $<$stdio.\+h$>$}\newline
{\ttfamily \#include $<$string.\+h$>$}\newline
{\ttfamily \#include $<$unistd.\+h$>$}\newline
{\ttfamily \#include $<$string$>$}\newline
{\ttfamily \#include $<$filesystem$>$}\newline
{\ttfamily \#include $<$stdlib.\+h$>$}\newline
{\ttfamily \#include "{}tflite/linux-\/jetson-\/nano/libeitrt.\+h"{}}\newline
{\ttfamily \#include $<$linux/limits.\+h$>$}\newline
Include dependency graph for tensorrt.\+h\+:
% FIG 0
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
bool \mbox{\hyperlink{tensorrt_8h_a8424f9ac492fd773762c19c595970e72}{file\+\_\+exists}} (char \texorpdfstring{$\ast$}{*}model\+\_\+file\+\_\+name)
\item 
\mbox{\hyperlink{group__ei__returntypes_gad9580b47a6cd5e74cfc31a03bdfecebe}{EI\+\_\+\+IMPULSE\+\_\+\+ERROR}} \mbox{\hyperlink{tensorrt_8h_a945f35e39a49bfe4e1194edaf89571cd}{run\+\_\+nn\+\_\+inference}} (const \mbox{\hyperlink{ei__model__types_8h_a936cf405a057a578f2bc7b540ccafe57}{ei\+\_\+impulse\+\_\+t}} \texorpdfstring{$\ast$}{*}impulse, \mbox{\hyperlink{structei__feature__t}{ei\+\_\+feature\+\_\+t}} \texorpdfstring{$\ast$}{*}fmatrix, uint32\+\_\+t learn\+\_\+block\+\_\+index, uint32\+\_\+t \texorpdfstring{$\ast$}{*}input\+\_\+block\+\_\+ids, uint32\+\_\+t input\+\_\+block\+\_\+ids\+\_\+size, \mbox{\hyperlink{structei__impulse__result__t}{ei\+\_\+impulse\+\_\+result\+\_\+t}} \texorpdfstring{$\ast$}{*}\mbox{\hyperlink{group__ei__functions_gaf4ad914acba713176b1f00a800e781ba}{result}}, void \texorpdfstring{$\ast$}{*}\mbox{\hyperlink{ei__run__dsp_8h_a0819d397f9fba7b345b75fef1c1f782a}{config\+\_\+ptr}}, bool \mbox{\hyperlink{ei__fill__result__struct_8h_a881f62341b5a41a3c5707268bd537be1}{debug}}=false)
\begin{DoxyCompactList}\small\item\em Do neural network inferencing over the processed feature matrix. \end{DoxyCompactList}\item 
\mbox{\hyperlink{group__ei__returntypes_gad9580b47a6cd5e74cfc31a03bdfecebe}{EI\+\_\+\+IMPULSE\+\_\+\+ERROR}} \mbox{\hyperlink{tensorrt_8h_a5086db080c7196118d01d4a4c4a1bd3c}{run\+\_\+nn\+\_\+inference\+\_\+image\+\_\+quantized}} (const \mbox{\hyperlink{ei__model__types_8h_a936cf405a057a578f2bc7b540ccafe57}{ei\+\_\+impulse\+\_\+t}} \texorpdfstring{$\ast$}{*}impulse, \mbox{\hyperlink{group__ei__structs_gaa11c321d559d65dd1adff6af0edf54e4}{signal\+\_\+t}} \texorpdfstring{$\ast$}{*}\mbox{\hyperlink{classei_1_1signal}{signal}}, \mbox{\hyperlink{structei__impulse__result__t}{ei\+\_\+impulse\+\_\+result\+\_\+t}} \texorpdfstring{$\ast$}{*}\mbox{\hyperlink{group__ei__functions_gaf4ad914acba713176b1f00a800e781ba}{result}}, void \texorpdfstring{$\ast$}{*}\mbox{\hyperlink{ei__run__dsp_8h_a0819d397f9fba7b345b75fef1c1f782a}{config\+\_\+ptr}}, bool \mbox{\hyperlink{ei__fill__result__struct_8h_a881f62341b5a41a3c5707268bd537be1}{debug}}=false)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
Ei\+Trt \texorpdfstring{$\ast$}{*} \mbox{\hyperlink{tensorrt_8h_a0ba202b686ba77088bbb6bc1cca24992}{ei\+\_\+trt\+\_\+handle}} = NULL
\end{DoxyCompactItemize}


\doxysubsection{Function Documentation}
\Hypertarget{tensorrt_8h_a8424f9ac492fd773762c19c595970e72}\index{tensorrt.h@{tensorrt.h}!file\_exists@{file\_exists}}
\index{file\_exists@{file\_exists}!tensorrt.h@{tensorrt.h}}
\doxysubsubsection{\texorpdfstring{file\_exists()}{file\_exists()}}
{\footnotesize\ttfamily \label{tensorrt_8h_a8424f9ac492fd773762c19c595970e72} 
bool file\+\_\+exists (\begin{DoxyParamCaption}\item[{char \texorpdfstring{$\ast$}{*}}]{model\+\_\+file\+\_\+name}{}\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}

\Hypertarget{tensorrt_8h_a945f35e39a49bfe4e1194edaf89571cd}\index{tensorrt.h@{tensorrt.h}!run\_nn\_inference@{run\_nn\_inference}}
\index{run\_nn\_inference@{run\_nn\_inference}!tensorrt.h@{tensorrt.h}}
\doxysubsubsection{\texorpdfstring{run\_nn\_inference()}{run\_nn\_inference()}}
{\footnotesize\ttfamily \label{tensorrt_8h_a945f35e39a49bfe4e1194edaf89571cd} 
\mbox{\hyperlink{group__ei__returntypes_gad9580b47a6cd5e74cfc31a03bdfecebe}{EI\+\_\+\+IMPULSE\+\_\+\+ERROR}} run\+\_\+nn\+\_\+inference (\begin{DoxyParamCaption}\item[{const \mbox{\hyperlink{ei__model__types_8h_a936cf405a057a578f2bc7b540ccafe57}{ei\+\_\+impulse\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{impulse}{, }\item[{\mbox{\hyperlink{structei__feature__t}{ei\+\_\+feature\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{fmatrix}{, }\item[{uint32\+\_\+t}]{learn\+\_\+block\+\_\+index}{, }\item[{uint32\+\_\+t \texorpdfstring{$\ast$}{*}}]{input\+\_\+block\+\_\+ids}{, }\item[{uint32\+\_\+t}]{input\+\_\+block\+\_\+ids\+\_\+size}{, }\item[{\mbox{\hyperlink{structei__impulse__result__t}{ei\+\_\+impulse\+\_\+result\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{result}{, }\item[{void \texorpdfstring{$\ast$}{*}}]{config\+\_\+ptr}{, }\item[{bool}]{debug}{ = {\ttfamily false}}\end{DoxyParamCaption})}



Do neural network inferencing over the processed feature matrix. 


\begin{DoxyParams}[1]{Parameters}
 & {\em fmatrix} & Processed matrix \\
\hline
 & {\em result} & Output classifier results \\
\hline
\mbox{\texttt{ in}}  & {\em debug} & Debug output enable\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The ei impulse error. 
\end{DoxyReturn}
\Hypertarget{tensorrt_8h_a5086db080c7196118d01d4a4c4a1bd3c}\index{tensorrt.h@{tensorrt.h}!run\_nn\_inference\_image\_quantized@{run\_nn\_inference\_image\_quantized}}
\index{run\_nn\_inference\_image\_quantized@{run\_nn\_inference\_image\_quantized}!tensorrt.h@{tensorrt.h}}
\doxysubsubsection{\texorpdfstring{run\_nn\_inference\_image\_quantized()}{run\_nn\_inference\_image\_quantized()}}
{\footnotesize\ttfamily \label{tensorrt_8h_a5086db080c7196118d01d4a4c4a1bd3c} 
\mbox{\hyperlink{group__ei__returntypes_gad9580b47a6cd5e74cfc31a03bdfecebe}{EI\+\_\+\+IMPULSE\+\_\+\+ERROR}} run\+\_\+nn\+\_\+inference\+\_\+image\+\_\+quantized (\begin{DoxyParamCaption}\item[{const \mbox{\hyperlink{ei__model__types_8h_a936cf405a057a578f2bc7b540ccafe57}{ei\+\_\+impulse\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{impulse}{, }\item[{\mbox{\hyperlink{group__ei__structs_gaa11c321d559d65dd1adff6af0edf54e4}{signal\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{signal}{, }\item[{\mbox{\hyperlink{structei__impulse__result__t}{ei\+\_\+impulse\+\_\+result\+\_\+t}} \texorpdfstring{$\ast$}{*}}]{result}{, }\item[{void \texorpdfstring{$\ast$}{*}}]{config\+\_\+ptr}{, }\item[{bool}]{debug}{ = {\ttfamily false}}\end{DoxyParamCaption})}

Special function to run the classifier on images for quantized models that allocates a lot less memory by quantizing in place. This only works if \textquotesingle{}can\+\_\+run\+\_\+classifier\+\_\+image\+\_\+quantized\textquotesingle{} returns EI\+\_\+\+IMPULSE\+\_\+\+OK. 

\doxysubsection{Variable Documentation}
\Hypertarget{tensorrt_8h_a0ba202b686ba77088bbb6bc1cca24992}\index{tensorrt.h@{tensorrt.h}!ei\_trt\_handle@{ei\_trt\_handle}}
\index{ei\_trt\_handle@{ei\_trt\_handle}!tensorrt.h@{tensorrt.h}}
\doxysubsubsection{\texorpdfstring{ei\_trt\_handle}{ei\_trt\_handle}}
{\footnotesize\ttfamily \label{tensorrt_8h_a0ba202b686ba77088bbb6bc1cca24992} 
Ei\+Trt\texorpdfstring{$\ast$}{*} ei\+\_\+trt\+\_\+handle = NULL}

