<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Machine Vision using Portenta H7: README</title>
<link rel="icon" href="logo.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">Machine Vision using Portenta H7<span id="projectnumber">&#160;2</span>
   </div>
   <div id="projectbrief">This project aims to develop a face recognition-based access control system using the Arduino Portenta H7 and Vision Shield, leveraging Edge Impulse for machine learning. The system captures facial images, processes them locally using an AI model deployed on the Portenta H7 and determines access based on authorised personnel.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',false);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div id="doc-content">
<div><div class="header">
  <div class="headertitle"><div class="title">README</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="autotoc_md270"></a>
Micro Speech Example</h1>
<p>This example shows how to run inference using TensorFlow Lite Micro (TFLM) on two models for wake-word recognition. The first model is an audio preprocessor that generates spectrogram data from raw audio samples. The second is the Micro Speech model, a less than 20 kB model that can recognize 2 keywords, "yes" and "no", from speech data. The Micro Speech model takes the spectrogram data as input and produces category probabilities.</p>
<h2><a class="anchor" id="autotoc_md271"></a>
Table of contents</h2>
<ul>
<li>Audio Preprocessor</li>
<li>Micro Speech Model Architecture</li>
<li>Run the C++ tests on a development machine</li>
<li>Run the evaluate.py script on a development machine</li>
<li>Run the evaluate_test.py script on a development machine</li>
<li>Converting models or audio samples to C++</li>
<li>Train your own model</li>
</ul>
<h2><a class="anchor" id="autotoc_md272"></a>
Audio Preprocessor</h2>
<p>The Audio Preprocessor model converts raw audio samples into a spectrographic feature. Audio samples are input to the model in windowed frames, each window overlapping the previous. When sufficient features have been accumulated, those features can be provided as input to the Micro Speech model.</p>
<p>This model provides a replication of the legacy preprocessing used during training of the Micro Speech model. For additional information on audio preprocessing during training, please refer to the <a href="train/README.md#preprocessing-speech-input">training README</a> documentation.</p>
<p>Audio Preprocessing models providing <code>int8</code> and <code>float32</code> output, ready for use with the Micro Speech model, are provided in the <a href="models/">models</a> directory. These models expect the audio input to conform to:</p><ul>
<li>30ms window frame</li>
<li>20ms window stride</li>
<li>16KHz sample rate</li>
<li>16-bit signed PCM data</li>
<li>single channel (mono)</li>
</ul>
<h3><a class="anchor" id="autotoc_md273"></a>
Model Architecture</h3>
<p>This model consists primarily of <a href="https://github.com/tensorflow/tflite-micro/blob/main/python/tflite_micro/signal">Signal Library</a> operations. The library is a set of Python methods, and bindings to <code>C++</code> library code. To allow for use with the <code>TFLM MicroInterpreter</code>, a set of <a href="https://github.com/tensorflow/tflite-micro/blob/main/signal/micro/kernels">Signal Library kernels</a> is also provided.</p>
<p>The <a href="audio_preprocessor.py">audio_preprocessor.py</a> script provides a complete example of how to use the <code>Signal Library</code> within your own Python application. This script has support for TensorFlow eager-execution mode, graph-execution mode, and <code>TFLM MicroInterpreter</code> inference operations.</p>
<p><a href="images/audio_preprocessor_int8.png"><img src="images/audio_preprocessor_int8.png" alt="model architecture" width="900" class="inline"/></a></p>
<p><em>This image was derived from visualizing the 'models/audio_preprocessor_int8.tflite' file in <a href="https://github.com/lutzroeder/netron">Netron</a></em></p>
<p>Each of the steps performed by the model are outlined as follows: 1) Audio frame input with shape <code>(1, 480)</code> 1) Apply <code>Hann Window</code> smoothing using <code>SignalWindow</code> 1) Reshape tensor to match the input of <code>SignalFftAutoScale</code> 1) Rescale tensor data using <code>SignalFftAutoScale</code> and calculate one of the input parameters to <code>SignalFilterBankSquareRoot</code> 1) Compute FFT using <code>SignalRfft</code> 1) Compute power spectrum using <code>SignalEnergy</code>. The tensor data is only updated for elements between <code>[start_index, end_index)</code>. 1) The <code>Cast</code>, <code>StridedSlice</code>, and <code>Concatenation</code> operations are used to fill the tensor data with zeros, for elements outside of <code>[start_index, end_index)</code> 1) Compress the power spectrum tensor data into just 40 channels (frequency bands) using <code>SignalFilterBank</code> 1) Scale down the tensor data using <code>SignalFilterBankSquareRoot</code> 1) Apply noise reduction using <code>SignalFilterBankSpectralSubtraction</code> 1) Apply gain control using <code>SignalPCAN</code> 1) Scale down the tensor data using <code>SignalFilterBankLog</code> 1) The remaining operations perform additional legacy down-scaling and convert the tensor data to <code>int8</code> 1) Model output has shape <code>(40,)</code></p>
<h3><a class="anchor" id="autotoc_md274"></a>
The <code>FeatureParams</code> Python Class</h3>
<p>The <code>FeatureParams</code> class is located within the <a href="audio_preprocessor.py#L260">audio_preprocessor.py</a> script. This class allows for custom configuration of the <code>AudioPreprocessor</code> class. Parameters such as sample rate, window size, window stride, number of output channels, and many more can be configured. The parameters to be changed must be set during class instantiation, and are frozen thereafter. The defaults for <code>FeatureParams</code> match those of the legacy audio preprocessing used during Micro Speech model training.</p>
<h3><a class="anchor" id="autotoc_md275"></a>
The <code>AudioPreprocessor</code> Python Class</h3>
<p>The <code>AudioPreprocessor</code> class in the <a href="audio_preprocessor.py#L338">audio_preprocessor.py</a> script provides easy to use convenience methods for creating and using an audio preprocessing model. This class is configured through use of a <code>FeatureParams</code> object, allowing some flexibility in how the audio preprocessing model works.</p>
<p>A short summary of the available methods and properties:</p><ul>
<li><code>load_samples</code>: load audio samples from a <code>WAV</code> format file and prepare the samples for use by other <code>AudioPreprocessor</code> methods</li>
<li><code>samples</code>: tensor containing previously loaded audio samples</li>
<li><code>params</code>: the <code>FeatureParams</code> object the class was instantiated with</li>
<li><code>generate_feature</code>: generate a single feature using TensorFlow eager-execution</li>
<li><code>generate_feature_using_graph</code>: generate a single feature using TensorFlow graph-execution</li>
<li><code>generate_feature_using_tflm</code>: generate a single feature using the <code>TFLM MicroInterpreter</code></li>
<li><code>reset_tflm</code>: reset the internal state of the <code>TFLM MicroInterpreter</code> and the <code>Signal Library</code> operations</li>
<li><code>generate_tflite_file</code>: create a <code>.tflite</code> format file for the preprocessor model</li>
</ul>
<h3><a class="anchor" id="autotoc_md276"></a>
Run the audio_preprocessor.py script on a development machine</h3>
<p>The <a href="audio_preprocessor.py#L532">audio_preprocessor.py</a> script generates a <code>.tflite</code> file for the preprocessing model, ready for use with the Micro Speech model.</p>
<p>To generate a <code>.tflite</code> model file with <code>int8</code> output: </p><div class="fragment"><div class="line">bazel build tensorflow/lite/micro/examples/micro_speech:audio_preprocessor</div>
<div class="line">bazel-bin/tensorflow/lite/micro/examples/micro_speech/audio_preprocessor --output_type=int8</div>
</div><!-- fragment --><p>To generate a <code>.tflite</code> model file with <code>float32</code> output: </p><div class="fragment"><div class="line">bazel build tensorflow/lite/micro/examples/micro_speech:audio_preprocessor</div>
<div class="line">bazel-bin/tensorflow/lite/micro/examples/micro_speech/audio_preprocessor --output_type=float32</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md277"></a>
Run the audio_preprocessor_test.py script on a development machine</h3>
<p>The <a href="audio_preprocessor_test.py">audio_preprocessor_test.py</a> script performs several tests to ensure correct inference operations occur across all execution modes. The tests are:</p><ul>
<li>cross-check inference results between eager, graph, and <code>TFLM MicroInterpreter</code> execution modes</li>
<li>check the <code>yes</code> and <code>no</code> 30ms samples in the <a href="testdata/">testdata</a> directory for correct generation of the feature tensor</li>
<li>compare the preprocessor <code>int8</code> model against the same model in the <a href="models/">models</a> directory</li>
<li>compare the preprocessor <code>float32</code> model against the same model in the <a href="models/">models</a> directory</li>
</ul>
<div class="fragment"><div class="line">bazel build tensorflow/lite/micro/examples/micro_speech:audio_preprocessor_test</div>
<div class="line">bazel-bin/tensorflow/lite/micro/examples/micro_speech/audio_preprocessor_test</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md278"></a>
Micro Speech Model Architecture</h2>
<p>This is a simple model comprised of a Convolutional 2D layer, a Fully Connected Layer or a MatMul Layer (output: logits) and a Softmax layer (output: probabilities) as shown below. Refer to the <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/models.py#L673"><code>tiny_conv</code></a> model architecture. The output probabilities are in four categories: <code>silence</code>, <code>unknown</code>, <code>yes</code>, <code>no</code>.</p>
<p>The input to the model is 49 spectrographic features, each feature consisting of 40 channels of data. The features are generated by the Audio Preprocessor model. For more information, please see the <a href="train/README.md#preprocessing-speech-input">training README</a> documentation.</p>
<p><a href="images/micro_speech_quantized.png"><img src="images/micro_speech_quantized.png" alt="model architecture" width="900" class="inline"/></a></p>
<p><em>This image was derived from visualizing the 'models/micro_speech_quantized.tflite' file in <a href="https://github.com/lutzroeder/netron">Netron</a></em></p>
<h2><a class="anchor" id="autotoc_md279"></a>
Run the C++ tests on a development machine</h2>
<p>To compile and test this example on a desktop Linux or macOS machine, download the <a href="https://github.com/tensorflow/tflite-micro">TFLM source code</a>. Then switch into the source directory from a terminal using the <code>cd</code> command.</p>
<p>Compile and run a native binary using Bazel: </p><div class="fragment"><div class="line">bazel run tensorflow/lite/micro/examples/micro_speech:micro_speech_test</div>
</div><!-- fragment --><p>For a native binary using <code>make</code>, run the following command: </p><div class="fragment"><div class="line">make -f tensorflow/lite/micro/tools/make/Makefile test_micro_speech_test</div>
</div><!-- fragment --><p>For an Arm Cortex-M0 binary running in the QEMU emulator: </p><div class="fragment"><div class="line">make -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex_m_qemu TARGET_ARCH=cortex-m0 OPTIMIZED_KERNEL_DIR=cmsis_nn BUILD_TYPE=default test_micro_speech_test</div>
</div><!-- fragment --><p>This will take a few minutes, and downloads frameworks the code uses like <a href="https://developer.arm.com/embedded/cmsis">CMSIS</a> and <a href="https://google.github.io/flatbuffers/">flatbuffers</a>. Once that process has finished, you should see a series of files get compiled, followed by some logging output from a test, which should conclude with <code>~~~ALL TESTS PASSED~~~</code>.</p>
<p>If you see this, it means that a small program has been built and executed that loads the trained TensorFlow Lite model, runs some example inputs through it, and got the expected outputs.</p>
<p>To understand how TFLM does this, you can look at the source in the <a href="micro_speech_test.cc">micro_speech_test.cc</a> file. It's a fairly small amount of code that executes the following steps: 1) Create a <code>TFLM MicroInterpreter</code> with a handle to the Audio Preprocessor model that has been compiled into the program 1) Repeatedly execute inference operations using <code>MicroInterpreter::invoke</code>, with audio samples as input, and spectrogram features as output 1) Create a new <code>TFLM MicroInterpreter</code> with a handle to the Micro Speech model that has been compiled into the program 1) Execute a single inference operation using <code>MicroInterpreter::invoke</code>, with the spectrogram features as input, and category probabilities as output 1) Check the largest category probability for a match with the speech sample label.</p>
<h2><a class="anchor" id="autotoc_md280"></a>
Run the evaluate.py script on a development machine</h2>
<p>The <a href="evaluate.py#L166">evaluate.py</a> script predicts the category of a single audio sample given by the <code>sample_path</code> argument. The output consists of the predictions for the accumulated spectrogram features across (at most) 49 audio sample window frames.</p>
<div class="fragment"><div class="line">bazel build tensorflow/lite/micro/examples/micro_speech:evaluate</div>
<div class="line">bazel-bin/tensorflow/lite/micro/examples/micro_speech/evaluate --sample_path=tensorflow/lite/micro/examples/micro_speech/testdata/no_1000ms.wav</div>
</div><!-- fragment --><p>The output looks like this: </p><div class="fragment"><div class="line">Frame #0: [0.0000, 0.0273, 0.0312, 0.9414]</div>
<div class="line">Frame #1: [0.0000, 0.0273, 0.0312, 0.9414]</div>
<div class="line">Frame #2: [0.0000, 0.0273, 0.0312, 0.9414]</div>
<div class="line">Frame #3: [0.0000, 0.0273, 0.0273, 0.9414]</div>
<div class="line">Frame #4: [0.0000, 0.0273, 0.0273, 0.9414]</div>
<div class="line">Frame #5: [0.0000, 0.0273, 0.0273, 0.9414]</div>
<div class="line">Frame #6: [0.0000, 0.0273, 0.0273, 0.9453]</div>
<div class="line">Frame #7: [0.0000, 0.0273, 0.0273, 0.9453]</div>
<div class="line">Frame #8: [0.0000, 0.0273, 0.0273, 0.9453]</div>
<div class="line"> </div>
<div class="line">...</div>
<div class="line"> </div>
<div class="line">Frame #40: [0.0000, 0.0312, 0.0000, 0.9648]</div>
<div class="line">Frame #41: [0.0000, 0.0273, 0.0000, 0.9727]</div>
<div class="line">Frame #42: [0.0000, 0.0312, 0.0000, 0.9688]</div>
<div class="line">Frame #43: [0.0000, 0.0273, 0.0000, 0.9727]</div>
<div class="line">Frame #44: [0.0000, 0.0273, 0.0000, 0.9727]</div>
<div class="line">Frame #45: [0.0000, 0.0352, 0.0000, 0.9648]</div>
<div class="line">Frame #46: [0.0000, 0.0391, 0.0000, 0.9609]</div>
<div class="line">Frame #47: [0.0000, 0.0469, 0.0000, 0.9531]</div>
<div class="line">Frame #48: [0.0000, 0.0547, 0.0000, 0.9453]</div>
<div class="line">Model predicts the audio sample as &lt;no&gt; with probability 0.95</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md281"></a>
Run the evaluate_test.py script on a development machine</h2>
<p>The <a href="evaluate_test.py">evaluate_test.py</a> script verifies the combination of the Audio Preprocessor model and the Micro Speech model to generate correct inference results. Four audio samples from the <a href="testdata/">testdata</a> directory are used as input to the Audio Preprocessor model. The Audio Preprocessor model is tested with both <code>int8</code> and <code>float32</code> outputs. The results of the audio preprocessing are then used to check predictions by the Micro Speech model.</p>
<div class="fragment"><div class="line">bazel build tensorflow/lite/micro/examples/micro_speech:evaluate_test</div>
<div class="line">bazel-bin/tensorflow/lite/micro/examples/micro_speech/evaluate_test</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md282"></a>
Converting models or audio samples to C++</h2>
<p>A tool is available to convert your custom model or audio samples into <code>C++</code> data structures that you can then use in your own wake-word application. Keep in mind that audio samples for use with Audio Preprocessor and Micro Speech models must be 1000ms in length, 16-bit PCM samples, and single channel (mono). The tool can be found here: <a href="../../tools/generate_cc_arrays.py">generate_cc_arrays.py</a></p>
<p>The following commands show how to use the tool: </p><div class="fragment"><div class="line">bazel build tensorflow/lite/micro/tools:generate_cc_arrays</div>
<div class="line">bazel-bin/tensorflow/lite/micro/tools/generate_cc_arrays /tmp/data.cc path_to_custom_sample.wav</div>
<div class="line">bazel-bin/tensorflow/lite/micro/tools/generate_cc_arrays /tmp/header.h path_to_custom_sample.wav</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md283"></a>
Train your own model</h2>
<p>So far you have used an existing trained model to run inference on microcontrollers. If you wish to train your own model, follow the instructions given in the <a class="el" href="md__arduino_2_get_started_with_machine_learning_on_arduino_2tflite-micro-main_2tensorflow_2lite_6a55a41b5fb6ca08272abc21546efeaa.html">train</a> directory. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
