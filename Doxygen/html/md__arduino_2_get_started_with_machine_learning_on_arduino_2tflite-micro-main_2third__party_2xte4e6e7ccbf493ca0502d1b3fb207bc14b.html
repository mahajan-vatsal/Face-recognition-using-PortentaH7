<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Machine Vision using Portenta H7: Mini Speech Training with LSTM</title>
<link rel="icon" href="logo.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">Machine Vision using Portenta H7<span id="projectnumber">&#160;2</span>
   </div>
   <div id="projectbrief">This project aims to develop a face recognition-based access control system using the Arduino Portenta H7 and Vision Shield, leveraging Edge Impulse for machine learning. The system captures facial images, processes them locally using an AI model deployed on the Portenta H7 and determines access based on authorised personnel.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',false);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div id="doc-content">
<div><div class="header">
  <div class="headertitle"><div class="title">Mini Speech Training with LSTM</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md365"></a></p>
<p>This example shows how to train a 125 kB model that can recognize any of 2 keywords from the below 8 keywords chosen by the user, classify all other commands as an "unknown" keyword, and predict the chosen keywords from speech data.</p>
<p>You can retrain it to recognize any combination of words (2 or more) from this list (all other words would be passed to "unknown" keyword set):</p>
<div class="fragment"><div class="line">&quot;down&quot;, &quot;go&quot;, &quot;left&quot;, &quot;no&quot;, &quot;right&quot;, &quot;stop&quot;, &quot;up&quot; and &quot;yes&quot;.</div>
</div><!-- fragment --><p>The scripts used in training the model have been sourced from the <a href="https://www.tensorflow.org/tutorials/audio/simple_audio">Simple Audio Recognition</a> tutorial.</p>
<h1><a class="anchor" id="autotoc_md366"></a>
Table of contents</h1>
<ul>
<li>Overview</li>
<li>Training</li>
<li>Trained Models</li>
<li>Model Architecture</li>
<li>Dataset</li>
<li>Preprocessing Speech Input</li>
</ul>
<h1><a class="anchor" id="autotoc_md367"></a>
Overview</h1>
<ol type="1">
<li>Dataset: <a href="http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip">Mini Speech Commands</a></li>
<li>Dataset Type: <b>Mini_Speech_Commands</b></li>
<li>Deep Learning Framework: <b>TensorFlow 2.5.0</b></li>
<li>Language: <b>Python 3.7</b></li>
<li>Model Size: **&lt;125 kB**</li>
<li>Model Category: **Multiclass Classification**</li>
</ol>
<h1><a class="anchor" id="autotoc_md368"></a>
Training</h1>
<p>Train the model in the cloud using Google Colaboratory.</p>
<table class="tfo-notebook-buttons" align="left">
</table>
<p><a href="https://colab.research.google.com/github/tensorflow/tflite-micro/blob/main/third_party/xtensa/examples/micro_speech_lstm/train/micro_speech_with_lstm_op.ipynb" target="_blank"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" alt="" class="inline"/>Google Colaboratory</a>  </p>
<p><em>Estimated Training Time: ~2 Minutes.</em></p>
<h1><a class="anchor" id="autotoc_md369"></a>
autotoc_md369</h1>
<h1><a class="anchor" id="autotoc_md370"></a>
Trained Models</h1>
<p>The flatbuffer model generated as a result of the traning can be found <a href="../micro_speech_lstm.tflite">here</a>. This model is quantized to int8 precision, i.e. all the activations and weights are int8.</p>
<h1><a class="anchor" id="autotoc_md371"></a>
Model Architecture</h1>
<p>This is a simple model comprising of a Unidirectional Sequence LSTM layer, a Reshape layer, a Fully Connected Layer or a MatMul Layer (output: logits) and a Softmax layer (output: probabilities) as shown below. Refer to the below model architecture.</p>
<p><img src="../images/lstm_model.png" alt="micro_speech_lstm_model" class="inline"/></p>
<p><em>This image was derived from visualizing the 'micro_speech_model.tflite' file in <a href="https://github.com/lutzroeder/netron">Netron</a></em></p>
<p>This produces a model with an accuracy of ~93%, but it's designed to be used as the first stage of a pipeline, running on a low-energy piece of hardware that can always be on, and then wake higher-power chips when a possible utterance has been found, so that more accurate analysis can be done. Additionally, the model takes in preprocessed speech input as a result of which we can leverage a simpler model for accurate results.</p>
<h1><a class="anchor" id="autotoc_md372"></a>
Dataset</h1>
<p>The <a href="http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip">Mini Speech Commands Dataset</a> consists of over 8,000 WAVE audio files of people saying 8 different words. This data was collected by Google and released under a CC BY license. You can help improve it by contributing five minutes of your own voice. The archive is over 2GB, so this part may take a while, but you should see progress logs, and once it's been downloaded you won't need to do this again.</p>
<h1><a class="anchor" id="autotoc_md373"></a>
Preprocessing Speech Input</h1>
<p>In this section we discuss spectrograms, the preprocessed speech input to the model. Here's an illustration of the process:</p>
<p><img src="../images/spectrogram.png" alt="Spectrogram LSTM" class="inline"/></p>
<p>The model doesn't take in raw audio sample data, instead it works with spectrograms which are two dimensional arrays that are made up of slices of frequency information, each taken from a different time window.</p>
<p>The recipe for creating the spectrogram data is that each frequency slice is created by running an FFT across a 30ms section of the audio sample data. The input samples are treated as being between -1 and +1 as real values (encoded as -32,768 and 32,767 in 16-bit signed integer samples).</p>
<p>This results in an FFT with 257 entries.</p>
<p>In a complete application these spectrograms would be calculated at runtime from microphone inputs, but the code for doing that is not yet included in this sample code. The test uses spectrograms that have been pre-calculated from one-second WAV files. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
